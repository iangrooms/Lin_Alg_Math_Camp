{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Review of some multivariate calculus\n",
    "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$.\n",
    "We can define the directional derivative using 1D calculus: Pick a unit vector $\\vec{u}\\in\\mathbb{R}^n$ (it defines a direction); then the **directional derivative** of $f$ in the direction $\\vec{u}$ evaluated at $\\vec{x}$ is (assuming the limit exists)\n",
    "\n",
    "$$\\lim_{\\epsilon\\to0}\\frac{f(\\vec{x}+\\epsilon\\vec{u})-f(\\vec{x})}{\\epsilon}.$$\n",
    "\n",
    "The directional derivative in the direction of a standard basis vector $\\vec{e}_i$ is denoted\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i}.$$\n",
    "\n",
    "It's also called the **partial derivative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If the function $f$ is sufficiently nice, then it has a gradient. The gradient is a linear function $L:\\mathbb{R}^n\\to\\mathbb{R}$ such that\n",
    "\n",
    "$$\\lim_{\\|\\vec{h}\\|\\to0}\\frac{\\|f(\\vec{x}+\\vec{h}) - f(\\vec{x}) - L[\\vec{h}]\\|}{\\|\\vec{h}\\|} = 0.$$\n",
    "\n",
    "This is an example of a Frechet derivative. It is assumed that the limit exists for all sequences $\\vec{h}_n\\to\\vec{0}$. Since all norms on $\\mathbb{R}^n$ are equivalent, you don't have to specify the norm with respect to which $\\vec{h}_n\\to\\vec{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The gradient is an element of the dual space of $\\mathbb{R}^n$, which means that can write it as a matrix (in this case a row vector), and the entries of the matrix depend on the basis you choose for $\\mathbb{R}^n$. If you use the standard basis then the entries of the matrix are the partial derivatives\n",
    "\n",
    "$$\\left(\\nabla f\\right)_i = \\frac{\\partial f}{\\partial x_i}.$$\n",
    "\n",
    "If we add an inner product on $\\mathbb{R}^n$ then we can invoke the Riesz representation theorem to write the directional derivative as the inner product of a vector with the direction. Using the standard basis and the dot product, the vector that defines the directional derivative in the direction $\\vec{u}$ is related to the gradient via\n",
    "\n",
    "$$\\text{directional derivative }=(\\nabla f)\\cdot\\vec{u}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now suppose that we have a vector $\\vec{f}$ of $m$ functions $\\vec{f}_i(\\vec{x})$, $i=1,\\ldots,m$.\n",
    "We can take the gradient of each scalar function $\\vec{f}_i$ and arrange them as rows of a matrix\n",
    "\n",
    "$$\\mathbf{J}_{ij} = \\frac{\\partial \\vec{f}_i}{\\partial x_j}.$$\n",
    "\n",
    "This matrix is called the **Jacobian** of $\\vec{f}$. If $\\vec{u}$ is a unit vector, then the $i^\\text{th}$ entry of the vector $\\mathbf{J}\\vec{u}$ is the directional derivative of $\\vec{f}_i$ in the direction $\\vec{u}$.\n",
    "\n",
    "If $\\vec{f} = \\nabla g$ then the Jacobian of $\\vec{f}$ is the **Hessian** of $g$, i.e. the matrix of second partial derivatives:\n",
    "\n",
    "$$\\mathbf{H}_{ij} = \\frac{\\partial^2 g}{\\partial x_i\\partial x_j}.$$\n",
    "\n",
    "The Hessian matrix is always symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Suppose we want to minimize a function $f(\\vec{x})$ with no constraints on allowable values of $\\vec{x}$.\n",
    "If $\\vec{f}$ is smooth enough, then the minimum (if it exists) has to occur at a critical point, i.e. a point where\n",
    "\n",
    "$$\\nabla f = \\vec{0}.$$\n",
    "\n",
    "If the gradient is zero, then the derivative is zero in every direction. Just like in 1D, some critical points are minima, some are maxima, and others are neither.\n",
    "In $n$ dimensions a point where the gradient is zero but that's neither a max nor a min is called a **saddle point**.\n",
    "\n",
    "Just like in 1D you can check whether a critical point is a max or min (or saddle) by checking the second derivative. In 1D if the second derivative is positive, then the critical point is a minimum; if it's negative then it's a maximum; otherwise it's inconclusive.\n",
    "\n",
    "In $n$ dimensions the critical point is a minimum if the Hessian is positive definite and it's a maximum if the Hessian is negative definite. If the Hessian is indefinite then it's a saddle, and if the Hessian is semi-definite then the test is inconclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Quadratic functions are very important in linear regression. A **quadratic function** has the form\n",
    "\n",
    "$$f(\\vec{x}) = \\vec{x}^T\\mathbf{K}\\vec{x} + \\vec{x}^T\\vec{b} + c.$$\n",
    "\n",
    "Let's compute the gradient of a quadratic function.\n",
    "First re-write it as\n",
    "\n",
    "$$f(\\vec{x}) = \\sum_{ij} K_{ij}x_ix_j + \\sum_i x_ib_i + c.$$\n",
    "\n",
    "Now take the derivative with respect to $x_k$, treating everything else as a constant:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_k} = \\sum_j K_{kj}x_j + \\sum_{i}K_{ik}x_i + b_k.$$\n",
    "\n",
    "Now re-write using vector notation, noticing that the first two sums are matrix/vector products\n",
    "\n",
    "$$\\nabla f =\\mathbf{K}\\vec{x} + \\mathbf{K}^T\\vec{x} + \\vec{b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Linear Least Squares\n",
    "Solving a linear system $\\mathbf{A}\\vec{x} = \\vec{b}$ can be thought of as trying to find the input $\\vec{x}$ to the function $f(\\vec{x}) = \\mathbf{A}\\vec{x}$ that gives the desired output $\\vec{b}$.\n",
    "\n",
    "If you put in the wrong input vector $\\vec{x}$, there are two kinds of error:\n",
    "1. **Error** usually means the difference between $\\vec{x}$ and the correct input $\\vec{x}_*$:   $\\quad \\vec{e} = \\vec{x}_*-\\vec{x}$\n",
    "2. The **residual** is the difference between the output $\\mathbf{A}\\vec{x}$ and the correct output $\\vec{b}$: $\\quad \\vec{r} = \\vec{b}-\\mathbf{A}\\vec{x}$.\n",
    "\n",
    "These two kinds of error are related via\n",
    "\n",
    "$$\\mathbf{A}\\vec{e} = \\vec{r}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now suppose that the linear system $\\mathbf{A}\\vec{x} = \\vec{b}$ has no solution. \n",
    "Often this happens when $\\mathbf{A}$ has more rows than columns.\n",
    "\n",
    "Clearly the error for input $\\vec{x}$ can't be defined, because there is no true solution.\n",
    "But interestingly, the *residual* can still be defined $\\vec{r} = \\vec{b} - \\mathbf{A}\\vec{x}$.\n",
    "Even though there's no input that will set the residual to zero, we can still try to find an input that minimizes the norm of the residual.\n",
    "\n",
    "$$\\text{find }\\vec{x}\\text{ that minimizes }\\|\\vec{r}\\|=\\|\\vec{b}-\\mathbf{A}\\vec{x}\\|.$$\n",
    "\n",
    "Naturally you have to pick a norm, and the answer depends on the norm you choose.\n",
    "The 2-norm is a special norm, because $\\|\\vec{b}-\\mathbf{A}\\vec{x}\\|_2^2$ is a quadratic function of $\\vec{x}$:\n",
    "\n",
    "$$\\|\\vec{b}-\\mathbf{A}\\vec{x}\\|_2^2 = (\\vec{b}-\\mathbf{A}\\vec{x})^T(\\vec{b}-\\mathbf{A}\\vec{x})=\\vec{x}^T\\mathbf{A}^T\\mathbf{A}\\vec{x} -2\\vec{x}^T\\mathbf{A}^T\\vec{b} + \\vec{b}^T\\vec{b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Compute the gradient of $\\vec{x}^T\\mathbf{A}^T\\mathbf{A}\\vec{x} -2\\vec{x}^T\\mathbf{A}^T\\vec{b} + \\vec{b}^T\\vec{b}$:\n",
    "\n",
    "$$\\nabla(\\|\\vec{r}\\|^2) = \\mathbf{A}^T\\mathbf{A}\\vec{x} + (\\mathbf{A}^T\\mathbf{A})^T\\vec{x} - 2(\\mathbf{A}^T\\vec{b})$$\n",
    "$$=2\\mathbf{A}^T\\mathbf{A}\\vec{x}-2\\mathbf{A}^T\\vec{b}.$$\n",
    "\n",
    "If we set the gradient to zero we get what are called the **normal equations**\n",
    "\n",
    "$$\\mathbf{A}^T\\mathbf{A}\\vec{x} = \\mathbf{A}^T\\vec{b}.$$\n",
    "\n",
    "Even though we're minimizing a nonlinear function, in this special case finding a minimum reduces to solving a linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "When is the coefficient matrix $\\mathbf{A}^T\\mathbf{A}$ from the normal equations invertible? I.e. when are we guaranteed to have a unique solution of the least-squares problem?\n",
    "\n",
    "Recall that $\\mathbf{A}^T\\mathbf{A}$ is a Gram matrix, which is always symmetric and at least positive semi-definite. It is symmetric positive definite whenever the columns of $\\mathbf{A}$ are linearly independent (i.e. rank of $\\mathbf{A}$ equals the number of columns of $\\mathbf{A}$.) So a solution is guaranteed to exist if the columns of $\\mathbf{A}$ are independent.\n",
    "\n",
    "What if the columns of $\\mathbf{A}$ are not linearly independent? Consider the problem\n",
    "\n",
    "$$\\mathbf{A}^T\\vec{z} = \\vec{v}.$$\n",
    "A solution exists whenever $\\vec{v}$ is in the range of $\\mathbf{A}^T$. But for our problem $\\vec{v} = \\mathbf{A}^T\\vec{b}$, which is obviously in the range of $\\mathbf{A}^T$, so a solution always exists.\n",
    "Furthermore, the solution $\\vec{z}$ is in the co-range of $\\mathbf{A}^T$, which is the range of $\\mathbf{A}$. If we next try to solve $\\mathbf{A}\\vec{x} = \\vec{z}$ then a solution exists whenever $\\vec{x}$ is in the range of $\\mathbf{A}\\ldots$\n",
    "\n",
    "When the columns of $\\mathbf{A}$ are linearly dependent the solution exists, but is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Weighted least squares\n",
    "The function that is minimized in a least-squares problem is the sum of squares of errors (the residuals) in each equation:\n",
    "\n",
    "$$\\|\\mathbf{A}\\vec{x}-\\vec{b}\\|_2^2 = (\\text{error in equation } 1)^2 + \\ldots (\\text{error in equation } m)^2 = \\|\\vec{r}(\\vec{x})\\|_2^2.$$\n",
    "\n",
    "Sometimes you might want to weight some of the errors differently, e.g. find an $\\vec{x}$ that minimizes\n",
    "\n",
    "$$5(\\text{error in equation } 1)^2 + (\\text{error in equation } 2)^2 + \\ldots (\\text{error in equation } m)^2.$$\n",
    "\n",
    "In this example putting a weight of 5 on the error in equation 1 means that we think the errors in equation 1 are more costly than the errors in the other equations.\n",
    "\n",
    "We can still preserve the nice properties of the 2-norm (i.e. that the nonlinear optimization problem reduces to solving a linear system) if we use an inner product norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Any inner product norm of the residual can be written as\n",
    "\n",
    "$$\\|\\vec{r}(\\vec{x})\\|_K^2 = \\|\\mathbf{A}\\vec{x}-\\vec{b}\\|_K^2 = (\\mathbf{A}\\vec{x}-\\vec{b})^T\\mathbf{K}(\\mathbf{A}\\vec{x}-\\vec{b})$$\n",
    "\n",
    "where $\\mathbf{K}$ is a symmetric positive definite weight matrix. The linear system you have to solve to find the minimizer is\n",
    "\n",
    "$$\\mathbf{A}^T\\mathbf{KA}\\vec{x} = \\mathbf{A}^T\\mathbf{K}\\vec{b}.$$\n",
    "\n",
    "In the example on the previous page we used a diagonal $\\mathbf{K}$ with 5 in the first entry and 1 on all the other diagonals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Closest Point\n",
    "There's a connection between least-squares and something called the 'closest point' problem.\n",
    "\n",
    "Suppose that you have a subspace $W$ and a vector $\\vec{b}$ that is not in $W$. How do you find $\\vec{w}\\in W$ that is closest to $\\vec{b}$?\n",
    "\n",
    "First pick a norm to measure 'closeness' and then minimize $\\|\\vec{b}-\\vec{w}\\|$ over vectors $\\vec{w}\\in W$.\n",
    "This is a 'constrained optimization problem' and is beyond the scope of the class.\n",
    "\n",
    "*Except* in the special case where we are using an inner product norm (e.g. the 2-norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Then the function we're trying to minimize is\n",
    "\n",
    "$$\\|\\vec{b}-\\vec{w}\\|^2 = \\langle\\vec{b}-\\vec{w},\\vec{b}-\\vec{w}\\rangle = \\|\\vec{b}\\|^2-2\\langle\\vec{b},\\vec{w}\\rangle + \\|\\vec{w}\\|^2.$$\n",
    "\n",
    "Suppose that we have an basis $\\vec{w}_1,\\ldots,\\vec{w}_k$ for the subspace $W$ so that we can write $\\vec{w} = x_1\\vec{w}_1+\\ldots+x_k\\vec{w}_k = \\mathbf{A}\\vec{x}$. Plug this in:\n",
    "\n",
    "$$\\|\\vec{b}-\\vec{w}\\|^2 = \\|\\vec{b}-\\mathbf{A}\\vec{x}\\|^2$$\n",
    "\n",
    "If the norm is derived form an inner product then this is a quadratic function. To find the minimum we set the gradient equal to zero. If we're in $\\mathbb{R}^n$ and using the dot product we just get the normal equations\n",
    "\n",
    "$$\\mathbf{A}^T\\mathbf{A}\\vec{x} = \\mathbf{A}^T\\vec{b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Solving the closest point problem with the 2-norm is the same as solving a least-squares problem!\n",
    "\n",
    "Actually they're not *exactly* the same. If you solve the closest-point problem for $\\vec{x}$, then the closest point is $\\vec{w} = \\mathbf{A}\\vec{x}$, whereas the solution of the least-squares problem is just $\\vec{x}$.\n",
    "\n",
    "The subspace $W$ in the closest-point problem is the range of $\\mathbf{A}$ in the least-squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Closest Point and Orthogonal Projection\n",
    "Recall the problem of finding the orthogonal projection of a vector $\\vec{b}$ onto a subspace $W$.\n",
    "By definition the orthogonal projection is the vector $\\vec{w}\\in W$ that makes $\\vec{b}-\\vec{w}$ orthogonal to $W$.\n",
    "\n",
    "This ends up being the same as the closest point in $W$ to $\\vec{b}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In $\\mathbb{R}^n$ with the dot product, the closest point is obtained by solving the normal equations for\n",
    "\n",
    "$$\\vec{x} = \\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T\\vec{b}$$\n",
    "\n",
    "and then constructing $\\vec{w} = \\mathbf{A}\\vec{x}$:\n",
    "\n",
    "$$\\vec{w} = \\mathbf{A}\\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T\\vec{b}.$$\n",
    "\n",
    "This is the same as the formula for the projection of $\\vec{b}$ onto $W$ where the columns of $\\mathbf{A}$ are a basis for $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If $W$ is the range of $\\mathbf{A}$, then the least squares problem is like finding the orthogonal projection of $\\vec{b}$ onto the range of $\\mathbf{A}$.\n",
    "\n",
    "The difference is that the least squares problem is looking for $\\vec{x}$ not $\\vec{v}$, and they are related by $\\mathbf{A}\\vec{x} = \\vec{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lagrange Multipliers\n",
    "\n",
    "Suppose you want to maximize the function $f(x,y) = x^2 + y^2$ along the line $y = 2 x + 3$. You could eliminate $y$ and then optimize the resulting univariate function over $x$, but this approach doesn't always generalize easily.\n",
    "\n",
    "Another approach is Lagrange multipliers. Write the constraint as\n",
    "\n",
    "$$g(x,y) = y - (2x+3) = 0.$$\n",
    "\n",
    "At an optimal point the gradient of $f$ should be parallel (or antiparallel) to the gradient of $f$:\n",
    "\n",
    "$$\\nabla f = \\lambda \\nabla g.$$\n",
    "\n",
    "We can also form a new function $J(x,y,\\lambda) = f(x,y) - \\lambda g(x,y)$ and say $\\nabla J = \\vec{0}$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
