{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Linear Functions\n",
    "\n",
    "A linear function $L$ from vector space $V$ to vector space $W$ satisfies\n",
    "$$\\forall \\;\\vec{v}_1,\\vec{v}_2\\in V\\qquad L[\\vec{v}_1+\\vec{v}_2] = L[\\vec{v}_1] + L[\\vec{v}_2]$$\n",
    "$$\\forall \\;\\vec{v}\\in V\\text{ and }\\forall\\;c\\in\\mathbb{R}\\qquad L[c \\vec{v}] = cL[\\vec{v}_1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let $\\{\\vec{v}_j\\}_{j=1}^n$ and $\\{\\vec{w}_i\\}_{i=1}^m$ be bases for $V$ and $W$, respectively.\n",
    "\n",
    "We know that there are coefficients $a_{ij}$ such that\n",
    "\n",
    "$$L[\\vec{v}_j] = \\sum_i a_{ij}\\vec{w}_i$$\n",
    "\n",
    "Let $\\vec{v} = \\sum_j x_j\\vec{v}_j$ and $\\vec{w} = L[\\vec{v}] = \\sum_iy_i\\vec{w}_i$.\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\vec{w} = \\sum_jx_jL[\\vec{v}_j] = \\sum_j x_j(\\sum_i a_{ij}\\vec{w}_i) = \\sum_{i}(\\sum_j a_{ij}x_j)\\vec{w}_i = \\sum_i y_i \\vec{w}_i.$$\n",
    "\n",
    "$$\\text{Uniqueness of the coordinates }\\Rightarrow\\quad \\vec{y} = \\mathbf{A}\\vec{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If $V$ and $W$ are finite-dimensional, then every linear function from $V$ to $W$ can be identified with a matrix, though the entries of the matrix depend on the bases for $V$ and $W$.\n",
    "\n",
    "Suppose that you have a linear map $L:\\mathbb{R}^n\\to\\mathbb{R}^m$ and you want its matrix representation with respect to the standard basis for both spaces. How can you find it?\n",
    "\n",
    "The $j^{\\text{th}}$ column of $\\mathbf{A}$ is $L[\\vec{e}_j]$, where the latter is represented using the standard basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The set of linear functions from $V$ to $W$ is itself a vector space of dimension $m\\times n$. (Addition and scalar multiplication for linear functions are defined here in the obvious way.)\n",
    "\n",
    "The space of linear functions from $V$ to $\\mathbb{R}$ (or $\\mathbb{C}$ if $V$ is a complex vector space) is called the 'dual space' of $V$. If $V$ has dimension $n$ then the dual space also has dimension $n$, and every element of the dual space can be identified with a $1\\times n$ matrix (i.e. a row vector).\n",
    "\n",
    "Of course, the entries of the matrix depend on the basis that you choose for $V$.\n",
    "\n",
    "Linear functions from a vector space to $\\mathbb{R}$ (or $\\mathbb{C}$) are often called _linear forms_ or _linear functionals_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Linear Systems\n",
    "\n",
    "A linear system of equations is a system of $m$ equations for $n$ unknown variables $x_1,\\ldots,x_n$ that takes the following form\n",
    "\n",
    "\\begin{align}\n",
    "a_{11}x_1 + a_{12}x_2 + \\ldots + a_{1n} x_n &= b_1\\\\\n",
    "a_{21}x_1 + a_{22}x_2 + \\ldots + a_{2n} x_n &= b_2\\\\\n",
    "\\vdots\\qquad\\qquad &= \\vdots\\\\\n",
    "a_{m1}x_1 + a_{m2}x_2 + \\ldots + a_{mn} x_n &= b_m\n",
    "\\end{align}\n",
    "\n",
    "The unknowns can be organized into the elements of a vector $\\vec{x}$; the coefficients $a_{ij}$ can be organized into elements of a matrix $\\mathbf{A}$; the elements on the right hand side can be organized into elements of a vector $\\vec{b}$.\n",
    "With this notation the linear system can be written in the form\n",
    "\n",
    "$$\\mathbf{A}\\vec{x} = \\vec{b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A *solution* of a linear system is a vector $\\vec{x}$ that satisfies $\\mathbf{A}\\vec{x} = \\vec{b}$.\n",
    "A linear system can have\n",
    "1. No solution\n",
    "2. One unique solution\n",
    "3. Infinitely many solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can prove that there are no other options than (i) none, (ii) one, or (iii) infinitely many by the following argument.\n",
    "\n",
    "Suppose that there are two solutions $\\vec{x}_a$ and $\\vec{x}_b$. Then $\\alpha\\vec{x}_a + (1-\\alpha)\\vec{x}_b$ is also a solution for any scalar $\\alpha$, i.e. if there are two solutions then there are an infinite number of solutions.\n",
    "\n",
    "$$\\mathbf{A}(\\alpha\\vec{x}_a + (1-\\alpha)\\vec{x}_b)= \\alpha\\mathbf{A}\\vec{x}_a + (1-\\alpha)\\mathbf{A}\\vec{x}_b = \\alpha\\vec{b} + (1-\\alpha)\\vec{b} = \\vec{b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Inverse Matrices\n",
    "\n",
    "Suppose that the linear system $\\mathbf{A}\\vec{x} = \\vec{b}$ has only one solution for every possible right hand side $\\vec{b}$, leaving $\\mathbf{A}$ fixed.\n",
    "We can write the relationship between the solution and the right hand side as a function $\\vec{f}$ that takes the right hand side $\\vec{b}$ as an input, and returns the unique solution $\\vec{x}$ as an output\n",
    "\n",
    "$$\\vec{x} = \\vec{f}(\\vec{b}).$$\n",
    "\n",
    "As it turns out, this function is linear! To see this, suppose that we have two right-hand-side/solution pairs $\\mathbf{A}\\vec{x}_a = \\vec{b}_a$ and $\\mathbf{A}\\vec{x}_b = \\vec{b}_b$, and two scalars $\\alpha$ and $\\beta$. We want to show that \n",
    "\n",
    "$$\\vec{f}(\\alpha\\vec{b}_a+\\beta\\vec{b}_b) = \\alpha\\vec{f}(\\vec{b}_a) + \\beta\\vec{f}(\\vec{b}_b) = \\alpha\\vec{x}_a + \\beta\\vec{x}_b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "I.e., we want to show that if $\\alpha\\vec{b}_a+\\beta\\vec{b}_b$ is the right hand side, then $\\alpha\\vec{x}_a + \\beta\\vec{x}_b$ is the solution. Easy:\n",
    "\n",
    "$$\\mathbf{A}\\left(\\alpha\\vec{x}_a + \\beta\\vec{x}_b\\right) = \\alpha\\mathbf{A}\\vec{x}_a + \\beta\\mathbf{A}\\vec{x}_b = \\alpha\\vec{b}_a+\\beta\\vec{b}_b.$$\n",
    "\n",
    "This means that the function from right hand side to solution, being a linear function, can be written as a matrix. This matrix is called the inverse of $\\mathbf{A}$ and is written\n",
    "\n",
    "$$\\vec{x} = \\mathbf{A}^{-1}\\vec{b}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The inverse matrix has the property that $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}$.\n",
    "\n",
    "Invertible matrices $\\mathbf{A}$ have the property that the **homogeneous** system $\\mathbf{A}\\vec{x} = \\vec{0}$ has only the **trivial** solution $\\vec{x} = \\vec{0}$. This implies that if $\\mathbf{A}$ is invertible then its columns are linearly independent.\n",
    "\n",
    "Non-square matrices cannot be invertible.\n",
    "\n",
    "Properties:\n",
    "\n",
    "$$\\left(\\mathbf{A}^T\\right)^{-1} = \\left(\\mathbf{A}^{-1}\\right)^T,\\qquad \\left(\\mathbf{AB}\\right)^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}$$\n",
    "\n",
    "**Do not** 'divide by a matrix.' Order matters and it's not clear if $\\mathbf{A}/\\mathbf{B}$ means $\\mathbf{AB}^{-1}$ or $\\mathbf{B}^{-1}\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A linear system $\\mathbf{A}\\vec{x} = \\vec{b}$ might have no solutions, one solution, or infinitely many solutions. Suppose you multiply from the left by a matrix $\\mathbf{B}$ you get a new system\n",
    "\n",
    "$$\\mathbf{BA}\\vec{x} = \\mathbf{B}\\vec{b}$$\n",
    "\n",
    "which can be written as\n",
    "$$\\mathbf{C}\\vec{x} = \\vec{d}$$\n",
    "$$\\mathbf{C} = \\mathbf{BA},\\;\\;\\vec{d} = \\mathbf{B}\\vec{b}.$$\n",
    "\n",
    "When $\\mathbf{B}$ is invertible, the new linear system $\\mathbf{C}\\vec{x} = \\vec{d}$ has exactly the same solutions as the original system $\\mathbf{A}\\vec{x} = \\vec{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It's clear that if $\\vec{x}$ is a solution of $\\mathbf{A}\\vec{x} = \\vec{b}$ then it is also a solution of the new system $\\mathbf{BA}\\vec{x} = \\mathbf{B}\\vec{b}$.\n",
    "All the solutions of the old system are still solutions of the new system, but how do we know that muliplying by $\\mathbf{B}$ does not introduce new solutions?\n",
    "The new system can be re-written as follows\n",
    "\n",
    "$$\\mathbf{B}\\vec{y} = \\vec{0}\\text{ where }\\vec{y} = \\left(\\mathbf{A}\\vec{x} -\\vec{b}\\right).$$\n",
    "\n",
    "Since $\\mathbf{B}$ is invertible there is only one solution, $\\vec{y} = \\vec{0}$, i.e.\n",
    "\n",
    "$$\\mathbf{A}\\vec{x} = \\vec{b}.$$\n",
    "\n",
    "The same argument applies if the columns of $\\mathbf{B}$ are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Elimination:\n",
    "Start with the linear system $\\mathbf{A}\\vec{x} = \\vec{b}$.\n",
    "Both sides are column vectors.\n",
    "On both sides, multiply element $j$ by $\\alpha$ and add it to element $i$.\n",
    "In the resulting system all the equations are the same except for equation $i$, which is now\n",
    "\n",
    "$$(a_{i1}+\\alpha a_{j1})x_1 + (a_{i2}+\\alpha a_{j2})x_2 + \\ldots + (a_{in}+\\alpha a_{jn})x_n = b_i+\\alpha b_j.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The operation 'multiply element $i$ by $\\alpha$ and add it to element $j$' is a linear function from $\\mathbb{R}^m$ to $\\mathbb{R}^m$, so it corresponds to matrix multiplication. What is the corresponding matrix? To find out, apply the operation to the columns of the identity matrix.\n",
    "\n",
    "The result is an **elementary matrix** $\\mathbf{E}$ that looks like the identity except that it has $E_{ij} = \\alpha$.\n",
    "For example, if you multiply equation $1$ by $\\alpha$ and add to equation 2, this is the same as multiplying from the left by the matrix\n",
    "\n",
    "$$\\mathbf{E} = \\left[\\begin{array}{cccc}1&0&\\cdots&0\\\\\\alpha&1&\\ddots&\\vdots\\\\0&&\\ddots&0\\\\0&\\cdots&0&1\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "This matrix is invertible; it's inverse is exactly the same except it has $-\\alpha$ in the $ij$ entry. If you add $\\alpha$ times row $i$ to row $j$, then subtract $\\alpha$ times row $i$ from row $j$, you're back where you started.\n",
    "\n",
    "For example,\n",
    "\n",
    "$$\\mathbf{E}^{-1} = \\left[\\begin{array}{cccc}1&0&\\cdots&0\\\\-\\alpha&1&\\ddots&\\vdots\\\\0&&\\ddots&0\\\\0&\\cdots&0&1\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Permutation\n",
    "\n",
    "Consider an operation that re-arranges the entries of a vector. Specifically, define a permutation vector $\\vec{p}$ with entries $p_i$ that are a re-arrangement of the indices $i=1,\\ldots,n$. Then define a function $L:\\mathbb{R}^n\\to\\mathbb{R}^n$ via\n",
    "\n",
    "$$\\left(L[\\vec{x}]\\right)_i = x_{p_i}.$$\n",
    "\n",
    "For example let $n=3$ and $\\vec{p} = (3,1,2)^T$. Then $L[\\vec{x}] = (x_3,x_1,x_2)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As the notation suggests, $L$ is linear so its action can be represented by matrix multiplication. Using the standard basis on $\\mathbb{R}^n$ we find the matrix representation of $L$ by just applying $L$ to the standard basis vectors, i.e. the columns of the identity matrix.\n",
    "\n",
    "Since $\\vec{e}_j$ is the $j^{\\mbox{\\tiny th}}$ column of the identity, its $i^{\\mbox{\\tiny th}}$ entry is $\\delta_{ij}$. If we apply the permutation to the rows the result is $\\delta_{p_ij}$, so the matrix representation of the permutation function has entries\n",
    "\n",
    "$$\\left(\\mathbf{P}\\right)_{ij} = \\delta_{p_ij}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "This is clearly an invertible operation, so the matrix must also be invertible. What are the entries of the inverse matrix?\n",
    "\n",
    "We will prove that $\\mathbf{P}^T = \\mathbf{P}^{-1}$. First note that the entries of the claimed inverse are\n",
    "\n",
    "$$\\left(\\mathbf{P}^T\\right)_{ij} = \\delta_{ip_j}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now consider the product of this claimed-inverse with the permutation matrix. The entries of the product are\n",
    "\n",
    "$$\\left(\\mathbf{P}^T\\mathbf{P}\\right)_{ij} = \\sum_k\\left(\\mathbf{P}^T\\right)_{ik}\\left(\\mathbf{P}\\right)_{kj} = \\sum_k \\delta_{i p_k}\\delta_{p_kj}.$$\n",
    "\n",
    "We are multiplying two lists of numbers and then adding the products. The numbers in each list are all 0 except for one 1, so the sum will be 0 except when the two lists contain a 1 in the same spot, and then the sum will be 1.\n",
    "\n",
    "Under what conditions do the two lists contain a 1 in the same spot, i.e. for the same value of $k$? The first list has a 1 when $p_k=i$ and the second list has a 1 when $p_k = j$, so when $i=j$ they have a 1 in the same spot, i.e.\n",
    "\n",
    "$$\\left(\\mathbf{P}^T\\mathbf{P}\\right)_{ij} = \\sum_k\\left(\\mathbf{P}^T\\right)_{ik}\\left(\\mathbf{P}\\right)_{kj} = \\sum_k \\delta_{i p_k}\\delta_{p_kj} = \\delta_{ij}.$$\n",
    "\n",
    "QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#  Gaussian Elimination\n",
    "is an algorithm that is guaranteed (in exact arithmetic) to find all solutions, if they exist, or to tell whether there are no solutions. It is based on three fundamental actions:\n",
    "1. Multiply one equation by a scalar and add it to another equation\n",
    "2. Re-order the equations\n",
    "3. If you can solve for a single variable, plug it back in to the equations to get a smaller system of equations\n",
    "\n",
    "Gaussian elimination prescribes a precise order for these operations: First eliminate, moving top to bottom then left to right; then substitute in the reverse order.\n",
    "\n",
    "All operations are invertible, so they don't change the solution set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The second step of Gaussian Elimination re-orders the equations. In vector form we have an equation with a column vector on the left and another column vector on the right. Re-ordering the elements of the vectors is a linear operation, so it corresponds to matrix multiplication. The corresponding matrix is obtained by applying the linear operation to the columns of the identity matrix, and is called a 'permutation matrix.'\n",
    "\n",
    "'Elementary' permutation matrices just swap 2 rows. They are their own inverses. More general permutation matrices shuffle the elements and are not their own inverses.\n",
    "\n",
    "Multiplication from the right by the transpose of a permutation matrix permutes the columns of the matrix:\n",
    "\n",
    "$$\\mathbf{AP}^T = \\left(\\mathbf{PA}^T\\right)^T$$\n",
    "\n",
    "Swap rows and columns; permute rows (which were columns); swap rows back to colums.\n",
    "\n",
    "The inverse of a general permutation matrix is its transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Gaussian Elimination puts a matrix in Row Echelon Form:\n",
    "![A screenshot from Definition 1.38 on page 59 of the book](RowEchelonForm.png)\n",
    "\n",
    "The circled elements are called **pivots** and the number of pivots is called the **rank**. Although the values in the row echelon form are not unique, the *number* of pivots and their *location* does not depend on the way that you swap rows during Gaussian elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A matrix is in **reduced row echelon form** (RREF) if it is in row echelon form, with the additional property that the first nonzero entry of each row is equal to 1 and is the only nonzero entry of its column. \n",
    "\n",
    "The RREF of a matrix is unique and does not depend on the sequence of elementary row operations used to obtain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The variables in the linear system that correspond to pivot columns in the REF are called 'basic' variables while the other variables are called 'free' variables.\n",
    "\n",
    "The rank of an $m\\times n$ matrix is $\\le$ the smaller of $m$ and $n$.\n",
    "\n",
    "If there are no free variables then the only solution to $\\mathbf{A}\\vec{x} = \\vec{0}$ is $\\vec{x} = \\vec{0}$, which means that the columns of $\\mathbf{A}$ are linearly independent. In this case we say that $\\mathbf{A}$ has 'full column rank.'\n",
    "\n",
    "If there is a pivot in every row then we say that $\\mathbf{A}$ has 'full row rank.' In this case the linear system $\\mathbf{A}\\vec{x} = \\vec{b}$ has a solution for every $\\vec{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If the system has a solution **and** there are free variables, then there is an infinite number of solutions.\n",
    "\n",
    "If you set all the free variables to zero then you are left with a system for the basic variables whose coefficient matrix is upper triangular with nonzero diagonal. Since we're assuming a solution exists (any rows of 0 on the left hand side are matched by 0 on the right hand side), you can solve for the basic variables using substitution. This provides one solution.\n",
    "\n",
    "Now set one of the free variables to 1. It is now fixed (not unknown), so move that component from the left hand side to the right hand side. Set all the other free variables to zero. Again you're left with an upper triangular system for the free variables. If you solve it you get a different solution.\n",
    "\n",
    "The existence of two solutions implies the existence of an infinite number of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# LU Factorization\n",
    "We saw that each elimination step is equivalent to multiplying from the left by an elementary matrix $\\mathbf{E}$. Each permutation of the rows corresponds to multiplication by a permutation matrix. So the whole process corresponds to\n",
    "\n",
    "$$\\mathbf{E}_{n-1}\\mathbf{P}_{n-1}\\cdots\\mathbf{E}_1\\mathbf{P}_1\\mathbf{A} = \\mathbf{U}$$\n",
    "\n",
    "where $\\mathbf{U}$ is the row echelon form of $\\mathbf{A}$, which is not unique because there is more than one way to permute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "All of the $\\mathbf{E}_i$ and $\\mathbf{P}_i$ matrices are invertible, so Gaussian Elimination corresponds to factoring the matrix\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{P}_1^{-1}\\mathbf{E}_1^{-1}\\cdots\\mathbf{P}_{n-1}^{-1}\\mathbf{E}_{n-1}^{-1}\\mathbf{U}.$$\n",
    "\n",
    "You'll see in Numerical Analysis I (APPM 5600) that we can convert this to a factorization in the form\n",
    "\n",
    "$$\\mathbf{PA} = \\mathbf{LU}$$\n",
    "\n",
    "where **P** is a permutation matrix, **L** is lower-triangular with ones on the diagonal, and **U** is the REF of **A**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If you swap columns as well as rows (i.e. re-order the unknowns as well as the equations), the factorization has the form\n",
    "\n",
    "$$\\mathbf{PAQ}^T = \\mathbf{LU}$$\n",
    "\n",
    "where $\\mathbf{Q}$ is another permutation matrix that swaps the columns. Permuting columns is not necessary in exact arithmetic, but can be beneficial in the presence of roundoff. You might see this topic in Numerical Linear Algebra (APPM 5620)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Example\n",
    "\n",
    "Numpy does not have an lu decomposition, for reasons of byzantine OSS development. Scipy (which is built on numpy) has an lu decomposition routine though, which just calls Lapack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[0 1 2]\n",
      " [2 1 0]\n",
      " [1 0 1]]\n",
      "p:\n",
      " [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "L:\n",
      " [[ 1.   0.   0. ]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.5 -0.5  1. ]]\n",
      "U:\n",
      " [[2. 1. 0.]\n",
      " [0. 1. 2.]\n",
      " [0. 0. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "from scipy import linalg\n",
    "A = np.array([[0,1,2],[2,1,0],[1,0,1]])\n",
    "p, L, U = linalg.lu(A)\n",
    "print('A:\\n',A)\n",
    "print('p:\\n',p)\n",
    "print('L:\\n',L)\n",
    "print('U:\\n',U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If you want to use the LU factorization to solve systems, you should use some different scipy utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0,1,2],[2,1,0],[1,0,1]])\n",
    "lu, piv = linalg.lu_factor(A) # Computes permuted LU factorization\n",
    "                              # packing L and U into a single matrix to save space\n",
    "b = np.array([3,3,2]) # Construct RHS\n",
    "x = linalg.lu_solve((lu, piv), b)\n",
    "print('x:\\n',x) # correct solution is 1,1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Fundamental subspaces\n",
    "\n",
    "Every $m\\times n$ matrix is associated with four 'fundamental' subspaces: two subspaces in $\\mathbb{R}^n$ and two in $\\mathbb{R}^m$.\n",
    "1. The span of the columns is a subspace of $\\mathbb{R}^m$. It's called the **range**, the **image**, or the **column space**.\n",
    "2. The span of the rows is a subspace of $\\mathbb{R}^n$. It's called the **corange**, the **coimage**, or the **row space**. It is equal to the range of $\\mathbf{A}^T$.\n",
    "3. The set of all vectors $\\vec{x}$ such that $\\mathbf{A}\\vec{x}=0$ is a subspace of $\\mathbb{R}^n$. It's called the **kernel** or the **null space**.\n",
    "4. The set of all vectors $\\vec{y}$ such that $\\mathbf{A}^T\\vec{y}=0$ is a subspace of $\\mathbb{R}^m$. It's called the **cokernel** or **left null space**. It is the kernel of $\\mathbf{A}^T$.\n",
    "\n",
    "These definitions can be extended in the to linear functions between vector spaces.\n",
    "\n",
    "The linear system $\\mathbf{A}\\vec{x} = \\vec{b}$ has a solution iff $\\vec{b}$ is in the range of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Suppose that you have found a REF for $\\mathbf{A}$.\n",
    "- The pivot rows of the REF are a basis for the corange\n",
    "- The pivot columns **of** $\\mathbf{A}$ are a basis for the range\n",
    "- The free columns of the **RREF** are a basis for the kernel\n",
    "- There are many ways to find a basis for the co-kernel, but none of them directly use the REF/RREF. Just find a basis for the kernel of $\\mathbf{A}^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Consider the permuted LU factorization of the matrix\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{P}^T\\mathbf{LU}$$\n",
    "\n",
    "where both $\\mathbf{P}$ and $\\mathbf{L}$ are $m\\times m$ and $\\mathbf{U}$ (the REF) is $m\\times n$.\n",
    "\n",
    "Take the transpose\n",
    "\n",
    "$$\\mathbf{A}^T = \\mathbf{U}^T\\mathbf{L}^T\\mathbf{P}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The corange of $\\mathbf{A}$ contains every possible vector in the form $\\mathbf{A}^T\\vec{x} = \\mathbf{U}^T\\mathbf{L}^T\\mathbf{P}\\vec{x} = \\mathbf{U}^T\\vec{y}$ where\n",
    "\n",
    "$$\\vec{y} = \\mathbf{L}^T\\mathbf{P}\\vec{x}.$$\n",
    "\n",
    "Clearly every vector in the corange is a linear combination of the columns of $\\mathbf{U}^T$, i.e. of the rows of the REF of $\\mathbf{A}$. Furthermore, the nonzero rows of the REF are linearly independent, so they are a good candidate for a basis for the corange. If we defined the corange as every possible vector of the form $\\mathbf{U}^T\\vec{y}$ then the nonzero rows of the REF would definitely be a basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "But the corange of $\\mathbf{A}$ is defined using every possible $\\vec{x}$. If we consider all possible $\\vec{x}$ is that the same as considering all possible \n",
    "\n",
    "$$\\vec{y} = \\mathbf{L}^T\\mathbf{P}\\vec{x}?$$\n",
    "\n",
    "Yes: Because $\\mathbf{L}^T\\mathbf{P}$ is invertible, considering all possible $\\vec{x}$ is the same as considering all possible $\\vec{y}$, so we conclude that the nonzero rows of the REF of $\\mathbf{A}$ are a basis for the corange of $\\mathbf{A}$.\n",
    "\n",
    "We will find ways of finding bases for all 4 fundamental subspaces when we cover the singular value decomposition (SVD). The remainder of the proof (about the range, kernel, and cokernel) is a bit lengthy and therefore omitted - ask me and I will give you a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "From the foregoing we conclude\n",
    "- The dimension of the range equals the rank of the matrix\n",
    "- The dimension of the corange equals the rank of the matrix\n",
    "- The dimension of the kernel equals the number of columns minus the rank of the matrix\n",
    "- The dimension of the cokernel equals the number of rows minus the rank of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Block Matrix Manipulation\n",
    "You can also apply Gaussian Elimination block-wise. For example consider a matrix\n",
    "\n",
    "$$\\left[\\begin{array}{c|c}\\mathbf{A}&\\mathbf{B}\\\\\\hline\\mathbf{C}&\\mathbf{D}\\end{array}\\right].$$\n",
    "\n",
    "Assume that $\\mathbf{A}$ is invertible and eliminate the lower-left block:\n",
    "\n",
    "$$\\left[\\begin{array}{c|c}\\mathbf{I}&\\mathbf{0}\\\\\\hline-\\mathbf{CA}^{-1}&\\mathbf{I}\\end{array}\\right]\\left[\\begin{array}{c|c}\\mathbf{A}&\\mathbf{B}\\\\\\hline\\mathbf{C}&\\mathbf{D}\\end{array}\\right]= \\left[\\begin{array}{c|c}\\mathbf{A}&\\mathbf{B}\\\\\\hline\\mathbf{0}&\\mathbf{D}-\\mathbf{CA}^{-1}\\mathbf{B}\\end{array}\\right]$$\n",
    "\n",
    "The matrix $\\mathbf{D} - \\mathbf{CA}^{-1}\\mathbf{B}$ is called the Schur complement of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Superposition\n",
    "Suppose that we are trying to solve a linear system $\\mathbf{A}\\vec{x} = \\vec{b}$, but we don't know the matrix $\\mathbf{A}$. All that we know is that the pairs\n",
    "\n",
    "$$(\\vec{x}_p,\\vec{b}_p)_{p=1}^P$$\n",
    "\n",
    "all solve the system $\\mathbf{A}\\vec{x}_p = \\vec{b}_p$. If $\\vec{b}$ is in the span of $\\vec{b}_1,\\ldots,\\vec{b}_P$ then there are coefficients $c_p$ such that\n",
    "\n",
    "$$c_1\\vec{b}_1+\\ldots+c_P\\vec{b}_P = \\vec{b}.$$\n",
    "\n",
    "Then $c_1\\vec{x}_1 + \\ldots + c_P\\vec{x}_P$ is a solution:\n",
    "\n",
    "$$\\mathbf{A}(c_1\\vec{x}_1 + \\ldots + c_P\\vec{x}_P) = c_1\\mathbf{A}\\vec{x}_1+\\ldots+c_P\\mathbf{A}\\vec{x}_P = c_1\\vec{b}_1+\\ldots+c_P\\vec{b}_P=\\vec{b}.$$\n",
    "\n",
    "If $\\vec{b}\\in\\mathbb{R}^n$ then set $\\mathbf{B} = [\\vec{b}_1\\cdots\\vec{b}_P]$ and solve\n",
    "\n",
    "$$\\mathbf{B}\\vec{c} = \\vec{b}$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
