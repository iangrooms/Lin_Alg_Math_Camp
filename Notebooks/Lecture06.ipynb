{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Eigenvalues and Eigenvectors\n",
    "Let $\\mathbf{A}$ be a square matrix. If there is a scalar $\\lambda$ and a vector $\\vec{v}\\neq\\vec{0}$ such that\n",
    "\n",
    "$$\\mathbf{A}\\vec{v} = \\lambda\\vec{v}$$\n",
    "\n",
    "then we say that $\\lambda$ and $\\vec{v}$ are an eigenvalue/eigenvector pair for the matrix $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Re-write the eigenvalue/eigenvector equation as follows\n",
    "\n",
    "$$(\\mathbf{A}-\\lambda\\mathbf{I})\\vec{v} = \\vec{0}.$$\n",
    "\n",
    "The only way for the above linear system to have a nonzero solution $\\vec{v}$ is if the coefficient matrix is singular (i.e. not invertible), in which case there are an infinite number of nonzero solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Theorem: Every $\\mathbf{A}\\in\\mathbb{C}^{n\\times n}$ has (at least) one eigenvalue.\n",
    "\n",
    "Let $\\vec{v}\\neq\\vec{0}$ and consider the vectors $\\vec{v},\\mathbf{A}\\vec{v},\\ldots,\\mathbf{A}^n\\vec{v}$. Because they are $n+1$ vectors in an $n$-dimensional space, they must be linearly dependent, i.e. there are coefficients $c_i$ (not all zero) such that\n",
    "\n",
    "$$c_0\\vec{v} + c_1\\mathbf{A}\\vec{v} + \\ldots + c_n\\mathbf{A}^n\\vec{v}=\\vec{0}.$$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$(c_0\\mathbf{I} + c_1\\mathbf{A} + \\ldots + c_n\\mathbf{A}^n)\\vec{v}=p(\\mathbf{A})\\vec{v} = \\vec{0}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Factor the polynomial: $p(x) = c_n(x-x_1)\\cdots(x - x_n)$.\n",
    "\n",
    "$$c_n(\\mathbf{A} - x_1\\mathbf{I})\\cdots(\\mathbf{A} - x_n\\mathbf{I})\\vec{v} = \\vec{0}.$$\n",
    "\n",
    "Since $\\vec{v}\\neq\\vec{0}$, at least one of the factors must be singular (non-invertible). I.e. there is a number $x_i$ such that $\\mathbf{A}-x_i\\mathbf{I}$ is singular, i.e. one of the $x_i$ is an eigenvalue of $\\mathbf{A}$. (Note that if $c_n=0$ we can just replace $n$ by the largest nonzero coefficient and the argument still works.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Theorem: If $\\lambda_1,\\ldots,\\lambda_m$ are distinct eigenvalues of $\\mathbf{A}$ corresponding to eigenvectors $\\vec{v}_1,\\ldots,\\vec{v}_m$, then $\\vec{v}_1,\\ldots,\\vec{v}_m$ are linearly independent.\n",
    "\n",
    "Proof: Suppose that \n",
    "$$a_1\\vec{v}_1+\\ldots+a_m\\vec{v}_m=\\vec{0}.$$\n",
    "\n",
    "Multiply from the left by $(\\mathbf{A} - \\lambda_2\\mathbf{I})\\cdots(\\mathbf{A} - \\lambda_m\\mathbf{I})$\n",
    "\n",
    "$$a_1(\\lambda_1-\\lambda_2)\\cdots(\\lambda_1-\\lambda_m)\\vec{v}_1 = \\vec{0}\\Rightarrow a_1=0.$$\n",
    "\n",
    "Since the ordering was arbitrary, conclude that all $a_i$ must be zero, i.e. the vectors are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Corollary: An $n\\times n$ matrix cannot have more than $n$ eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The **geometric multiplicity** of an eigenvalue is the dimension of the nullspace of $\\mathbf{A} - \\lambda\\mathbf{I}$. (Sometimes called the 'number of eigenvectors'.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Theorem: The eigenvalues of $\\mathbf{A}$ are the same as the eigenvalues of $\\mathbf{A}^T$.\n",
    "\n",
    "Proof: The rank of any matrix equals the rank of its transpose. The eigenvalues and their geometric multiplicity are therefore preserved under transposition.\n",
    "\n",
    "Note that the _eigenvectors_ of a matrix are not the same as the eigenvectors of its transpose, and for complex matrices the eigenvalues are preserved under the _transpose_ but not the _conjugate transpose_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Complex eigenvalues of real matrices come in complex-conjugate pairs. Suppose that $\\mathbf{A}\\vec{v} = \\lambda\\vec{v}$. Then\n",
    "\n",
    "$$\\mathbf{A}\\vec{\\bar{v}} = \\bar{\\lambda}\\vec{\\bar{v}}$$\n",
    "\n",
    "i.e. $\\bar{\\lambda}$ is an eigenvalue with eigenvector $\\vec{\\bar{v}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "If the sum of the geometric multiplicities of all the eigenvalues of an $n\\times n$ matrix equals $n$, then the matrix is **diagonalizable**.\n",
    "\n",
    "Suppose that the matrix is diagonalizable. Then there are linearly independent eigenvectors\n",
    "\n",
    "$$\\mathbf{A}\\vec{v}_1 = \\lambda_1\\vec{v}_1,\\cdots,\\mathbf{A}\\vec{v}_n=\\lambda_n\\vec{v}_n$$\n",
    "\n",
    "(where there might be repeats in the list $\\lambda_1,\\ldots,\\lambda_n$.)\n",
    "\n",
    "Define the matrix $\\mathbf{S} = [\\vec{v}_1\\;\\cdots\\;\\vec{v}_n]$. It is $n\\times n$ and has linearly independent columns, so it is invertible. The list of vector equations above can be arranged as columns of a matrix equation:\n",
    "\n",
    "$$\\mathbf{AS} = \\mathbf{S\\Lambda}\\quad\\Rightarrow\\quad \\mathbf{A} = \\mathbf{S\\Lambda S}^{-1}.$$\n",
    "where $\\mathbf{\\Lambda}$ is diagonal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A nonzero vector $\\vec{w}$ such that $\\vec{w}^T\\mathbf{A} = \\lambda\\vec{w}^T$ is called a **left eigenvector** of **A**. Equivalently, $\\mathbf{A}^T\\vec{w} = \\lambda\\vec{w}$.\n",
    "\n",
    "If $\\mathbf{A} = \\mathbf{S\\Lambda S}^{-1}$ then the rows of $\\mathbf{S}^{-1}$ are left eigenvectors of **A**:\n",
    "\n",
    "$$\\mathbf{S}^{-1}\\mathbf{A}= \\mathbf{\\Lambda S}^{-1}$$\n",
    "\n",
    "The rows of the above matrix equation are in the form $\\vec{w}_i^T\\mathbf{A} = \\lambda_i\\vec{w}_i^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Square matrices **A** and **B** are **similar** when there is an invertible matrix **T** such that\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{TBT}^{-1}.$$\n",
    "\n",
    "Similar matrices have the same eigenvalues (but not eigenvectors).\n",
    "\n",
    "Proof: Suppose that $\\mathbf{B} -\\lambda\\mathbf{I}$ is singular. Then $\\mathbf{T}(\\mathbf{B}-\\lambda\\mathbf{I})\\mathbf{T}^{-1}=\\mathbf{A} -\\lambda\\mathbf{I}$ is also singular because multiplying by invertible matrices $\\mathbf{T}$ and $\\mathbf{T}^{-1}$ can't turn a singular matrix into a nonsingular one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Spectral theorem for normal matrices\n",
    "\n",
    "A matrix **A** is **normal** when it commutes with its complex-conjugate transpose:\n",
    "\n",
    "$$\\mathbf{A}^*\\mathbf{A} = \\mathbf{AA}^*.$$\n",
    "\n",
    "There exists a unitary matrix $\\mathbf{U}$ that diagonalizes $\\mathbf{A}$ iff **A** is normal. I.e. **A** has an orthonormal eigenvector basis iff it is normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "First consider the implication\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{UDU}^*\\;\\Rightarrow\\;\\mathbf{A}^*\\mathbf{A} = \\mathbf{AA}^*.$$\n",
    "\n",
    "If we insert the antecedent into the consequent and simplify we find\n",
    "\n",
    "$$\\mathbf{UD}^*\\mathbf{U}^*\\mathbf{UDU}^* = \\mathbf{UD}\\mathbf{U}^*\\mathbf{UD}^*\\mathbf{U}^*.$$\n",
    "\n",
    "Using the fact that $\\mathbf{U}$ is unitary leads to\n",
    "\n",
    "$$\\mathbf{UD}^*\\mathbf{DU}^* = \\mathbf{UD}\\mathbf{D}^*\\mathbf{U}^*.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Multiplying from the left by $\\mathbf{U}^*$ and from the right by $\\mathbf{U}$ leads to\n",
    "\n",
    "$$\\mathbf{D}^*\\mathbf{D} = \\mathbf{D}\\mathbf{D}^*.$$\n",
    "\n",
    "We want to know whether this is true, and it is: diagonal matrices commute. We conclude that \n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{UDU}^*\\;\\Rightarrow\\;\\mathbf{A}^*\\mathbf{A} = \\mathbf{AA}^*.$$\n",
    "\n",
    "(Perhaps a more clean proof would start with commuting diagonal matrices then re-introduce the unitary matrices until arriving at $\\mathbf{A}^*\\mathbf{A} = \\mathbf{AA}^*.$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now consider the reverse imlication: normal implies unitarily diagonalizable. The proof uses the Schur decomposition ([proof here](https://en.wikipedia.org/wiki/Schur_decomposition)); for every **A** there is a unitary matrix **U** such that\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{UTU}^*$$\n",
    "\n",
    "where **T** is upper-triangular. If **A** is normal then\n",
    "\n",
    "$$\\mathbf{T}^*\\mathbf{T} = \\mathbf{TT}^*$$\n",
    "\n",
    "which is only possible if $\\mathbf{T}$ is diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Consider the (1,1) element of the matrix equation on the previous slide. It says\n",
    "\n",
    "$$|t_{11}|^2 = |t_{11}|^2 + \\cdots+|t_{1n}|^2.$$\n",
    "\n",
    "This is only possible if $t_{1i}=0$ for $i\\ge 2$.\n",
    "\n",
    "Next consider the (2,2) element of the matrix equation, using what we know about the first row of $\\mathbf{T}$:\n",
    "\n",
    "$$|t_{22}|^2 = |t_{22}|^2 + \\cdots+|t_{2n}|^2.$$\n",
    "\n",
    "Once again, this is only possible if $t_{2i}=0$ for $i\\ge 3$. Continuing this line of argument shows that $\\mathbf{T}$ must be diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Real symmetric matrices and complex Hermitian matrices (are normal and) have real eigenvalues.\n",
    "Suppose that $\\mathbf{A}$ is Hermitian and it has a complex eigenvalue:\n",
    "$$\\mathbf{A}\\vec{v} = \\lambda\\vec{v}.$$\n",
    "Now take the complex dot product with $\\vec{v}$:\n",
    "$$\\vec{v}^T\\bar{\\mathbf{A}}\\vec{\\bar{v}} = \\bar{\\lambda}\\|\\vec{v}\\|^2.$$\n",
    "Now take the complex-conjugate transpose of both sides:\n",
    "$$\\vec{v}^T\\mathbf{A}^T\\vec{\\bar{v}} =\\vec{v}^T\\bar{\\mathbf{A}}\\vec{\\bar{v}} = \\lambda\\|\\vec{v}\\|^2.$$\n",
    "Together these equations imply that $\\lambda = \\bar{\\lambda}$, i.e. that $\\lambda$ is real. When **A** is real symmetric, the eigenvectors must also be real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For a real symmetric matrix there is an orthogonal matrix **Q** such that $\\mathbf{A} = \\mathbf{Q\\Lambda Q}^T$.\n",
    "\n",
    "Similarly, real skew-symmetric and complex anti-Hermitian matrices are normal and have pure imaginary eigenvalues (possibly including 0).\n",
    "\n",
    "Real orthogonal and complex unitary matrices are normal and have eigenvalues on the unit circle in the complex plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Eigenvalues and Symmetric Positive Definite Matrices\n",
    "Suppose that $\\mathbf{A}$ is a real symmetric matrix.\n",
    "If $\\vec{x}^T\\mathbf{A}\\vec{x}>0$ for every $\\vec{x}\\neq\\vec{0}$ then $\\mathbf{A}$ is *symmetric positive definite* (SPD). We can now give an alternative characterization of SPD matrices using eigenvalues:\n",
    "\n",
    "A real symmetric matrix is SPD when all its eigenvalues are positive:\n",
    "\n",
    "$$\\vec{x}^T\\mathbf{A}\\vec{x} = \\vec{x}^T\\mathbf{Q\\Lambda Q}^T\\vec{x} = \\vec{y}^T\\mathbf{\\Lambda}\\vec{y},\\qquad\\left(\\vec{y} = \\mathbf{Q}^T\\vec{x}\\right)$$\n",
    "$$=\\lambda_1 y_1^2+\\ldots+\\lambda_ny_n^2.$$\n",
    "\n",
    "If $\\lambda_i$ is negative or zero then we can make $\\vec{x}^T\\mathbf{A}\\vec{x}\\le0$ by setting $\\vec{y} = \\vec{e}_i$, i.e. $\\vec{x} = \\mathbf{Q}\\vec{e}_i\\neq\\vec{0}$.\n",
    "So the only way to get $\\vec{x}^T\\mathbf{A}\\vec{x}>0$ is to have all the eigenvalues be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The connection between \"Positive Definite\" ($\\vec{x}^T\\mathbf{A}\\vec{x}\\ge0$) and \"Real Positive Eigenvalues\" _only works for real symmetric and complex Hermitian matrices_. \n",
    "\n",
    "If you have a non-symmetric matrix with real positive eigenvalues it doesn't guarantee $\\vec{x}^T\\mathbf{A}\\vec{x}\\ge0$. Real positive eigenvalues doesn't even imply diagonalizable.\n",
    "\n",
    "Similarly, if you have a non-symmetric matrix that satisfies $\\vec{x}^T\\mathbf{A}\\vec{x}\\ge0$, it doesn't imply that the eigenvalues are real and positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Singular Values and the 2-norm\n",
    "Recall that the formula for the matrix norm induced by a vector norm is\n",
    "\n",
    "$$\\|\\mathbf{A}\\| = \\max_{\\|\\vec{u}\\|=1}\\|\\mathbf{A}\\vec{u}\\|.$$\n",
    "\n",
    "We have nice formulas for the 1-norm of $\\infty$-norm of a matrix, but not for the 2-norm. There is a nice formula for the 2-norm in terms of singular values.\n",
    "\n",
    "Consider the function \n",
    "$$\\|\\mathbf{A}\\vec{u}\\|_2^2 = \\vec{u}^T\\mathbf{A}^T\\mathbf{A}\\vec{u} = f(\\vec{u}).$$\n",
    "We want to optimize this function, but we also want have $\\|\\vec{u}\\|_2 = 1$.\n",
    "This is an *equality-constrained* optimization problem. We will use the method of *Lagrange multipliers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Lagrange Multipliers](LagrangeMultipliers2D.svg)\n",
    "\n",
    "(Image from [Wikipedia](https://en.wikipedia.org/wiki/Lagrange_multiplier#/media/File:LagrangeMultipliers2D.svg).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The idea of Lagrange multipliers is that you have some function $f(\\vec{u})$ that you want to optimize but you also have some constraint on the allowable values of $\\vec{u}$, given in the form $g(\\vec{u}) = 0$.\n",
    "\n",
    "The usual condition for an critical point is just $\\nabla f = \\vec{0}$; for the constrained problem the condition is\n",
    "\n",
    "$$\\nabla f = \\lambda\\nabla g$$\n",
    "\n",
    "where $\\lambda$ is some unknown number called a 'Lagrange multiplier.'\n",
    "\n",
    "In our problem $f(\\vec{u}) = \\vec{u}^T\\mathbf{A}^T\\mathbf{A}\\vec{u}$ and $g(\\vec{u}) = \\vec{u}^T\\vec{u} -1$, so the critical points are defined by\n",
    "\n",
    "$$2\\mathbf{A}^T\\mathbf{A}\\vec{u} = 2\\lambda\\vec{u}.$$\n",
    "\n",
    "Any pair $(\\lambda,\\vec{u})$ that solves this equation is a critical point, and clearly also an eigenvalue/vector pair for the matrix $\\mathbf{A}^T\\mathbf{A}$.\n",
    "We want the critical point that gives us the largest function value $f(\\vec{u})$; at a critical point we will have $f(\\vec{u}) = \\vec{u}^T\\mathbf{A}^T\\mathbf{A}\\vec{u} = \\lambda\\vec{u}^T\\vec{u} = \\lambda$.\n",
    "So the largest eigenvalue $\\lambda$ of $\\mathbf{A}^T\\mathbf{A}$ is the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Back to the 2-norm: \n",
    "$$\\|\\mathbf{A}\\|_2 = \\max_{\\|\\vec{u}\\|_2=1}\\|\\mathbf{A}\\vec{u}\\|_2 = \\sqrt{\\lambda_{\\text{max}}}$$\n",
    "where $\\lambda_{\\text{max}}$ is the largest eigenvalue of $\\mathbf{A}^T\\mathbf{A}$. This is not the usual notation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If $\\mathbf{A}$ is real and symmetric, the $\\mathbf{A}^T\\mathbf{A} = \\mathbf{A}^2$. \n",
    "\n",
    "In this case the eigenvalues of $\\mathbf{A}$ are all real, and the eigenvalues of $\\mathbf{A}^T\\mathbf{A} = \\mathbf{A}^2$ are the squares of eigenvalues of $\\mathbf{A}$; they are therefore real and non-negative.\n",
    "\n",
    "For a real symmetric matrix we conclude that\n",
    "\n",
    "$$\\|\\mathbf{A}\\|_2 = |\\lambda_{\\text{max}}|$$\n",
    "\n",
    "where $\\lambda_{\\text{max}}$ is the largest eigenvalue of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "**Review** Start with a real $m\\times n$ matrix $\\mathbf{A}$, then form the Gram matrix $\\mathbf{K} = \\mathbf{A}^T\\mathbf{A}$.\n",
    "\n",
    "$\\mathbf{K}$ is a symmetric positive (semi-)definite matrix, so it has real, non-negative eigenvalues and real eigenvectors. The eigenvectors can be chosen to be orthogonal, and in the context of SVD they are always chosen that way.\n",
    "\n",
    "The eigenvectors and eigenvalues of the Gram matrix satisfy\n",
    "\n",
    "$$\\mathbf{A}^T\\mathbf{A}\\vec{q}_i = \\sigma_i^2\\vec{q}_i,\\;i=1,\\ldots,n.$$\n",
    "\n",
    "To avoid confusion, call the eigenvectors $\\vec{q}_i$ of the Gram matrix **singular vectors** of $\\mathbf{A}$.\n",
    "\n",
    "Call the nonzero $\\sigma_i$ **singular values** of $\\mathbf{A}$ and order them so that $\\sigma_1\\ge\\sigma_2\\ge\\ldots\\ge\\sigma_s>0$. How many nonzero singular values are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The null space of $\\mathbf{A}$ is included in the null space of $\\mathbf{A}^T\\mathbf{A}$, and its dimension equals $n-r$ where $r$ is the rank of $\\mathbf{A}$.\n",
    "\n",
    "In fact, the only vectors $\\vec{q}_i$ with $\\mathbf{A}^T\\mathbf{A}\\vec{q}_i=\\vec{0}$ are in the null space of $\\mathbf{A}$:\n",
    "\n",
    "$\\mathbf{A}\\vec{q}_i$ is in the range of $\\mathbf{A}$. If $\\mathbf{A}^T(\\mathbf{A}\\vec{q}_i)=\\vec{0}$ then $\\mathbf{A}\\vec{q}_i$ is in the cokernel of $\\mathbf{A}$. The only vector that is in both the range and the cokernel is $\\mathbf{A}\\vec{q}_i=\\vec{0}$, which means that $\\vec{q}_i$ is in the kernel of $\\mathbf{A}$.\n",
    "\n",
    "We conclude that the number of (nonzero) singular values equals the rank of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Notice that the singular vectors $\\vec{q}_1,\\ldots,\\vec{q}_r$ are in the corange of $\\mathbf{A}$ by construction:\n",
    "\n",
    "$$\\vec{q}_i=\\sigma_i^{-2}\\mathbf{A}^T(\\mathbf{A}\\vec{q}_i).$$\n",
    "\n",
    "Since there are $r$ of them and the corange has dimension $r$, they must be an orthonormal basis for the corange of $\\mathbf{A}$.\n",
    "\n",
    "The remaining vectors $\\vec{q}_{r+1},\\ldots,\\vec{q}_n$ are an orthonormal basis for the kernel of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let\n",
    "$$\\vec{w}_i = \\mathbf{A}\\vec{q}_i,\\;i=1,\\ldots,r.$$\n",
    "\n",
    "The $\\vec{w}_i$ are all in the range of $\\mathbf{A}$.\n",
    "\n",
    "The singular vectors $\\vec{q}_i$ are not eigenvectors of $\\mathbf{A}$, so $\\vec{w}_i$ are not $\\sigma_i\\vec{q}_i$.\n",
    "But the $\\vec{w}_i$ are orthogonal:\n",
    "\n",
    "$$\\vec{w}_i\\cdot\\vec{w}_j = \\left(\\mathbf{A}\\vec{q}_i\\right)\\cdot\\left(\\mathbf{A}\\vec{q}_j\\right) = \\vec{q}_i^T\\mathbf{A}^T\\mathbf{A}\\vec{q}_i = \\sigma_i^2\\vec{q}_j\\cdot\\vec{q}_i$$\n",
    "\n",
    "If $i\\neq j$ then this is zero because the singular vectors are orthogonal. This shows that the $\\vec{w}_i$ are orthogonal.\n",
    "If $i=j$ then we have $\\vec{w}_i\\cdot\\vec{w}_i = \\|\\vec{w}_i\\|_2^2 = \\sigma_i^2$, so $\\|\\vec{w}_i\\|=\\sigma_i$.\n",
    "So we can write $\\vec{w}_i = \\sigma_i\\vec{p}_i$ where $\\vec{p}_i$ are orthonormal.\n",
    "\n",
    "The vectors $\\vec{p}_1,\\ldots,\\vec{p}_r$ are all in the range of $\\mathbf{A}$, and there are $r$ of them, so they must be an orthonormal basis for the range of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "At this point we have an orthonormal (o.n.) basis for $\\mathbb{R}^n$ $\\vec{q}_1,\\ldots,\\vec{q}_n$ where the first $r$ vectors are an o.n. basis for the corange of $\\mathbf{A}$ and the remaining vectors are an o.n. basis for the kernel of $\\mathbf{A}$.\n",
    "\n",
    "We also have an o.n. basis $\\vec{p}_1,\\ldots,\\vec{p}_r$ for the range of $\\mathbf{A}$. Let $\\vec{p}_{r+1},\\ldots,\\vec{p}_m$ be an o.n. basis for the cokernel of $\\mathbf{A}$.\n",
    "(Such a basis exists, and you can find one using random vectors in $\\mathbb{R}^m$ and Gram-Schmidt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Putting it all together we have\n",
    "\n",
    "$$\\mathbf{A}\\vec{q}_i = \\sigma_i\\vec{p}_i\\text{ for }i=1,\\ldots,r,\\text{ and }\\mathbf{A}\\vec{q}_i = \\vec{0}\\text{ for }i=r+1,\\ldots n.$$\n",
    "\n",
    "If we write these vector equations left to right as a matrix equation we have\n",
    "\n",
    "$$\\mathbf{AQ} = \\left[\\sigma_1\\vec{p}_1\\,\\cdots\\,\\sigma_r\\vec{p}_r\\,\\vec{0}\\,\\cdots\\,\\vec{0}\\right]$$\n",
    "\n",
    "where $\\mathbf{Q} = [\\vec{q}_1\\,\\cdots\\,\\vec{q}_n]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The matrix equation on the previous slide can be written\n",
    "\n",
    "$$\\mathbf{AQ} = \\mathbf{P\\Sigma}$$\n",
    "\n",
    "where $\\mathbf{P} = [\\vec{p}_1\\,\\cdots\\,\\vec{p}_m]$ and\n",
    "\n",
    "$$\\mathbf{\\Sigma} = \\left[\\begin{array}{cccccc}\\sigma_1&&&&&\\\\&\\ddots&&&&\\\\&&\\sigma_r&&&\\\\&&&0&&\\\\&&&&\\ddots&\\\\&&&&&0\\end{array}\\right]$$\n",
    "\n",
    "is $m\\times n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Since $\\mathbf{Q}$ is orthogonal we can multiply from the right by $\\mathbf{Q}^T$ to produce\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{P\\Sigma Q}^T$$\n",
    "\n",
    "which is the **singular value decomposition** (SVD) of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "There are multiple versions of the SVD that look the same except that the matrices are of different size.\n",
    "\n",
    "In the SVD we've just covered $\\mathbf{A}$ is $m\\times n$, $\\mathbf{Q}$ is $n\\times n$, $\\mathbf{\\Sigma}$ is $m\\times n$, and $\\mathbf{P}$ is $m\\times m$.\n",
    "\n",
    "There's also a 'thin' version of the SVD that looks the same:\n",
    "\n",
    "$$\\mathbf{A}=\\mathbf{P\\Sigma Q}^T$$\n",
    "\n",
    "but $\\mathbf{P}$ is $m\\times r$, $\\mathbf{\\Sigma}$ is $r\\times r$ and invertible, and $\\mathbf{Q}$ is $n\\times r$. You basically just delete the columns of $\\mathbf{P}$ and $\\mathbf{Q}$ that correspond to the cokernel and kernel of $\\mathbf{A}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The singular values of **A** are the same as the singular values of $\\mathbf{A}^T$, and the SVD of $\\mathbf{A}^T$ is\n",
    "\n",
    "$$\\mathbf{A}^T = \\mathbf{Q\\Sigma P}^T.$$\n",
    "\n",
    "The 2-norm of a square matrix is its largest singular value $\\sigma_1$. This definition extends to non-square matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# SVD and least squares\n",
    "\n",
    "Recall that when $\\mathbf{A}$ has full column rank, the solution of the linear least squares problem $\\mathbf{A}\\vec{x} = \\vec{b}$ is\n",
    "\n",
    "$$\\vec{x} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\vec{b}.$$\n",
    "\n",
    "Plugging in the (thin) SVD of $\\mathbf{A}$ and simplifying yields\n",
    "\n",
    "$$\\vec{x} = \\mathbf{Q\\Sigma}^{-1}\\mathbf{P}^T\\vec{b}.$$\n",
    "\n",
    "The matrix $\\mathbf{A}^+ = \\mathbf{Q\\Sigma}^{-1}\\mathbf{P}^T$ is called the Moore-Penrose pseudoinverse of $\\mathbf{A}$. If $\\mathbf{A}$ is invertible then $\\mathbf{A}^+=\\mathbf{A}^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "When $\\mathbf{A}$ does not have full column rank the least-squares problem does not have a unique solution.\n",
    "\n",
    "The solution set corresponds to a unique vector in the corange of $\\mathbf{A}$ plus any vector from the kernel of $\\mathbf{A}$.\n",
    "\n",
    "The solution that is just the unique vector in the corange of $\\mathbf{A}$ has the smallest 2-norm of all the solutions in the solution set.\n",
    "\n",
    "The solution obtained using the pseudoinverse is in the corange by construction. This sounds nice, and it is from a theoretical perspective, but when $\\mathbf{A}$ does not have full column rank you can't reliably compute its pseudoinverse, so this does not have much pratical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Eckart-Young Theorem\n",
    "Let $\\mathbf{A}=\\mathbf{P\\Sigma Q}^T$ be $m\\times n$. \n",
    "\n",
    "Define $\\mathbf{A}_k=\\mathbf{P}_k\\mathbf{\\Sigma}_k\\mathbf{Q}_k^T$ by taking the first $k$ columns of $\\mathbf{P}$ and $\\mathbf{Q}$, and the leading $k\\times k$ part of $\\mathbf{\\Sigma}$.\n",
    "\n",
    "$\\mathbf{A}_k$ is a minimizer of $\\|\\mathbf{A} - \\hat{\\mathbf{A}}\\|_F$ over all matrices $\\hat{\\mathbf{A}}$ with rank $\\le k$. (If $\\sigma_k > \\sigma_{k+1}$ then this is the unique minimizer.)\n",
    "\n",
    "The same is true if we replace the Frobenius norm with the matrix 2 norm. [Proof](https://en.wikipedia.org/wiki/Low-rank_approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Normal matrices and diagonalizability\n",
    "So far we can classify square matrices as\n",
    "- Normal\n",
    "- Diagonalizable but not normal\n",
    "- Not diagonalizable\n",
    "\n",
    "The distinction between the first two is a bit artificial for the following reason:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Update the definition of **normal** to mean **commutes with its adjoint**.\n",
    "\n",
    "Theorem: Every diagonalizable matrix is normal with respect to some inner product. (Proof: Exercise.)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
