{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e5786c-ebcd-4631-a424-ef5a4de72885",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Angle between vectors\n",
    "Cauchy-Schwarz implies the following for nonzero real vectors\n",
    "$$-1\\le \\frac{\\langle\\vec{x},\\vec{y}\\rangle}{\\|\\vec{x}\\|\\|\\vec{y}\\|}\\le 1.$$\n",
    "This allow us to define an angle between nonzero real vectors:\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\langle\\vec{x},\\vec{y}\\rangle}{\\|\\vec{x}\\|\\|\\vec{y}\\|},\\;\\;\\theta = \\cos^{-1}\\left(\\frac{\\langle\\vec{x},\\vec{y}\\rangle}{\\|\\vec{x}\\|\\|\\vec{y}\\|}\\right).$$\n",
    "\n",
    "Notice that the angle you compute depends on the inner product you use. The Cauchy-Schwarz inequality guarantees that the right hand side of the first expression is always between -1 and 1, so the inverse cosine is well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a626f97-fd5e-4a36-9962-3eca23d01ccb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Vectors that are perpendicular have a $90^\\circ$ angle between them, and the cosine of $90^\\circ$ is 0, so we say that vectors are **orthogonal** when\n",
    "\n",
    "$$\\langle\\vec{x},\\vec{y}\\rangle = 0.$$\n",
    "\n",
    "When the vectors are in $\\mathbb{R}^n$ or $\\mathbb{C}^n$ we sometimes use the word 'orthogonal' _only_ when the dot product is zero, and use the word 'conjugate' when some other inner product is zero.\n",
    "\n",
    "If someone uses the word 'orthogonal' when referring to vectors in $\\mathbb{R}^n$ or $\\mathbb{C}^n$, you should assume they are using the dot product unless explicitly told otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8e2ac-c6b6-4e72-bc68-1dc77c77af05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Inner products on complex vector spaces map from $V\\times V$ to $\\mathbb{C}$, so if you use the foregoing definition of the angle between vectors then the angle will be complex.\n",
    "\n",
    "There is no standard definition of the angle between complex vectors, but it is common to use\n",
    "\n",
    "$$\\theta = \\cos^{-1}\\left(\\frac{\\text{Real}\\left\\{\\langle\\vec{x},\\vec{y}\\rangle\\right\\}}{\\|\\vec{x}\\|\\|\\vec{y}\\|}\\right).$$\n",
    "\n",
    "We still say that complex vectors are orthogonal **only** when their inner product is 0, not when the angle between them is $\\pm\\pi/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf8b42-728c-4a68-8a32-7e10d71e8f43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A set of nonzero vectors that are mutually orthogonal is linearly independent. (If mutually orthogonal then linearly independent.)\n",
    "\n",
    "Assume $\\vec{v}_1,\\ldots,\\vec{v}_n$ are mutually orthogonal and nonzero. Consider a linear combination equal to zero:\n",
    "\n",
    "$$c_1\\vec{v}_1+\\ldots+c_n\\vec{v}_n=\\vec{0}.$$\n",
    "\n",
    "Now take the inner product of both sides with $\\vec{v}_i$ and use the fact that the vectors are orthogonal\n",
    "\n",
    "$$\\langle\\vec{v}_i,c_1\\vec{v}_1+\\ldots+c_n\\vec{v}_n\\rangle=c_1\\langle\\vec{v}_i,\\vec{v}_1\\rangle + \\ldots + c_i\\langle\\vec{v}_i,\\vec{v}_i\\rangle + \\ldots + c_n\\langle\\vec{v}_i,\\vec{v}_n\\rangle = c_i\\langle\\vec{v}_i,\\vec{v}_i\\rangle=0.$$\n",
    "\n",
    "Mutual orthogonality implies that all the coefficients must be zero, i.e. the vectors are linearly independent.\n",
    "\n",
    "A set of vectors that are mutually orthogonal and are also unit vectors (with respect to the norm derived from the inner product) are called **orthonormal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ef9e8-9544-4ebf-af1e-488a22156a1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Orthogonal Matrices\n",
    "\n",
    "Consider a real matrix $\\mathbf{Q}$. What does it mean when\n",
    "\n",
    "$$\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}?$$\n",
    "\n",
    "The matrix on the left is a Gram matrix consisting of dot products of the columns of $\\mathbf{Q}$. The matrix equation tells us that these columns are orthogonal (with respect to the dot product) and that their norm (2-norm) is 1.\n",
    "\n",
    "If $\\mathbf{Q}$ is square, then the equation also implies that $\\mathbf{Q}^T=\\mathbf{Q}^{-1}$, which also implies\n",
    "\n",
    "$$\\mathbf{QQ}^T = \\mathbf{I}.$$\n",
    "\n",
    "Real square matrices with $\\mathbf{Q}^T=\\mathbf{Q}^{-1}$ are called **orthogonal** matrices. It would make more sense to call them 'orthonormal' matrices, but we don't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20b443-8682-4fc1-a602-9443e824a894",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A _complex_ square matrix $\\mathbf{U}$ that satisfies\n",
    "\n",
    "$$\\mathbf{U}^T\\bar{\\mathbf{U}} = \\mathbf{I}$$\n",
    "\n",
    "is called a **unitary** matrix. The matrix on the left is the Gram matrix formed from the columns of $\\mathbf{U}$ using the complex dot product.\n",
    "\n",
    "The matrix on the right is real, so we can take the complex conjugate of both sides to get\n",
    "\n",
    "$$\\mathbf{U}^*\\mathbf{U} = \\mathbf{I}.$$\n",
    "\n",
    "This is _by far_ the more common way of defining a unitary matrix, but it loses the connection to the complex dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8b033-c70a-4fd7-9b3a-52303d386b8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Orthogonal subspaces\n",
    "\n",
    "Two subspaces $U$ and $V$ of an inner-product space $W$ are orthogonal when every vector in $U$ is orthogonal to every vector in $V$.\n",
    "\n",
    "Practically speaking if we have a basis $\\vec{u}_1,\\ldots,\\vec{u}_p$ of $U$ and a basis $\\vec{v}_1,\\ldots,\\vec{v}_q$ of $V$, then the subspaces are orthogonal when $\\langle\\vec{u}_i,\\vec{v}_j\\rangle = 0$ $\\forall i,j.$\n",
    "\n",
    "The **orthogonal complement** $U^\\perp$ of a subspace $U$ within a space $W$ is the set of vectors\n",
    "\n",
    "$$U^\\perp = \\{w\\in W : w\\perp U\\}.$$\n",
    "\n",
    "The orthogonal complement is itself a subspace (no proof)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fb219-f297-4cc7-961d-8db3b56fd1be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We already know that the linear system $\\mathbf{A}\\vec{x} = \\vec{b}$ only has a solution when the vector $\\vec{b}$ is in the range of $\\mathbf{A}$.\n",
    "\n",
    "Since the range is the orthogonal complement of the cokernel, we can now say that the linear system only has a solution when the vector $\\vec{b}$ is orthogonal to the cokernel of $\\mathbf{A}$.\n",
    "\n",
    "This is called the 'Fredholm alternative.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e733e-2dfa-456d-94bb-92b46d1be07c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Orthogonal Projections\n",
    "\n",
    "Suppose that $\\vec{v}$ is not in a subspace $W$. The **orthogonal projection** of $\\vec{v}$ onto $W$ is the vector $\\vec{w}\\in W$ that makes the difference $\\vec{z} = \\vec{v}-\\vec{w}$ orthogonal to $W$.\n",
    "\n",
    "(A vector is orthogonal to a subspace whenever it is orthogonal to every vector in that subspace. Equivalently, when it is orthogonal to every vector in a basis for that subspace.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9985c-9016-454e-9a08-d3b5f5ca6c85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Olver & Shakiban Figure 4.4](OS4p4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee52ee-1627-4461-a912-524fe34fd7d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "How can we compute the projection? Start with a basis $\\vec{w}_1,\\ldots,\\vec{w}_k$ for the subspace.\n",
    "Then require $\\vec{v}-\\vec{w}$ to be orthogonal to every one of the basis vectors:\n",
    "\n",
    "$$0=\\langle\\vec{w}_i,\\vec{v}-\\vec{w}\\rangle \\Rightarrow \\langle\\vec{w}_i,\\vec{v}\\rangle = \\langle\\vec{w}_i,\\vec{w}\\rangle.$$\n",
    "\n",
    "If we're using the dot product on $\\mathbb{R}^n$, this system can be written as \n",
    "\n",
    "$$\\mathbf{A}^T\\vec{w} = \\mathbf{A}^T\\vec{v}$$\n",
    "\n",
    "where the columns of $\\mathbf{A}$ are $\\vec{w}_1,\\ldots,\\vec{w}_k$.\n",
    "Now we want $\\vec{w}$ to be in the subspace, i.e.\n",
    "\n",
    "$$\\vec{w} = c_1\\vec{w}_1+\\ldots+c_k\\vec{w}_k = \\mathbf{A}\\vec{c},\\text{ so }\\mathbf{A}^T\\mathbf{A}\\vec{c} = \\mathbf{A}^T\\vec{v}.$$\n",
    "\n",
    "The general solution is \n",
    "$$\\vec{w} = \\mathbf{A}\\vec{c} = \\mathbf{A}\\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T\\vec{v}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce85e9-4664-460c-8fa8-3bda0c5ca40c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The matrix $\\mathbf{A}^T\\mathbf{A}$ is a Gram matrix and the columns of $\\mathbf{A}$ are a basis (implying that the columns are linearly independent), so the Gram matrix is invertible. The matrix $\\mathbf{A}\\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T$ is an example of an **orthogonal projection matrix** when the columns of $\\mathbf{A}$ are linearly independent. Despite the name, it is not an orthogonal matrix.\n",
    "\n",
    "This orthogonal projection matrix projects onto the range (also called the image or column space) of $\\mathbf{A}$.\n",
    "\n",
    "The projection of $\\vec{v}$ onto the cokernel of $\\mathbf{A}$ is $\\vec{v} - \\vec{w} = (\\mathbf{I}-\\mathbf{A}\\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T)\\vec{v}$. \n",
    "\n",
    "The matrix $\\mathbf{I}-\\mathbf{A}\\left(\\mathbf{A}^T\\mathbf{A}\\right)^{-1}\\mathbf{A}^T$ is also an orthogonal projection matrix, but it projects onto the orthogonal complement of the range of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3025b6-3e5b-4a7d-bc12-7cf3012a8bde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If $\\vec{w}_1,\\ldots,\\vec{w}_k$ are an orthonormal basis for $W$, then $\\mathbf{A}^T\\mathbf{A}=\\mathbf{I}$ and the solution is\n",
    "\n",
    "$$\\vec{w} = \\mathbf{A}\\vec{c} = \\mathbf{AA}^T\\vec{b}.$$\n",
    "\n",
    "If $\\mathbf{A}$ has orthonormal columns, then $\\mathbf{AA}^T$ is called an **orthogonal projection matrix**.\n",
    "\n",
    "This is the usual formula for an orthogonal projection matrix, where it's assumed that we already have an orthonormal basis available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9fbe4-e021-4088-af5f-504ae37ff9d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Every vector $\\vec{v}$ in the space $V$ can be written as the sum of a vector $\\vec{w}$ from the subspace $W$ and a vector $\\vec{z}$ from $W^\\perp$, i.e. the orthogonal complement of $W$.\n",
    "We say\n",
    "\n",
    "$$V = W \\oplus W^\\perp.$$\n",
    "\n",
    "Proof: $\\vec{w}$ is the orthogonal projection of $\\vec{v}$ into $W$, and $\\vec{z} = \\vec{v}-\\vec{w}$ is in $W^\\perp$ by the definition of the orthogonal projection.\n",
    "\n",
    "If the dimension of $V$ is $n$ and the dimension of $W$ is $r\\le n$, then the dimension of $W^\\perp$ must be $n-r$. (No proof, but it's not hard.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b3393-902f-451d-94b2-04f52e9a0771",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Theorem: The cokernel of $\\mathbf{A}$ is the orthogonal complement of the range of $\\mathbf{A}$. (Corollary: The kernel of $\\mathbf{A}$ is the orthogonal complement of the corange of $\\mathbf{A}$.)\n",
    "\n",
    "Let's show that every vector in the range is orthogonal to the cokernel.\n",
    "1. Suppose that $\\vec{v}\\in$ Range$(\\mathbf{A})$. This means that there is some $\\vec{x}$ (not necessarily unique) such that $\\vec{v} = \\mathbf{A}\\vec{x}$.\n",
    "2. Suppose that $\\vec{w}\\in$ Cokernel$(\\mathbf{A})$. This means that $\\vec{w}^T\\mathbf{A} = \\vec{0}^T$.\n",
    "3. Now consider the dot product $\\vec{w}^T\\vec{v}$. We can write this as $\\vec{w}^T\\mathbf{A}\\vec{x} = \\vec{0}^T\\vec{x}=0$. \n",
    "This shows that every vector in the range is orthogonal to every vector in the cokernel.\n",
    "\n",
    "To show that the subspaces are complements we need to show that together they span the whole space.\n",
    "To do this we use the fundamental theorem: The dimension of the range is $r$ and the dimension of the cokernel is $m-r$, so the dimensions add up to $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c2050-747f-4c16-820f-d0295691e825",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Gram-Schmidt\n",
    "\n",
    "So far we only know how to find a basis for the kernel of a matrix, and the basis that we find is not guaranteed to be orthogonal.\n",
    "Orthogonal bases are very useful, so we now look at a method that starts with a basis and generates an orthogonal basis.\n",
    "\n",
    "Suppose that we start with a non-orthogonal basis $\\vec{x}_1,\\ldots,\\vec{x}_n$, and we want to find an orthogonal basis for the same subspace.\n",
    "The Gram-Schmidt process (algorithm) does this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b535ad-bb8b-4d63-81fc-9decc617f14e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "1. $\\vec{v}_1 = \\vec{x}_1$.\n",
    "2. $\\vec{v}_2 = \\vec{x}_2 - c_{2,1}\\vec{v}_1$. We want to choose $c_{2,1}$ in such a way that $\\vec{v}_1$ and $\\vec{v}_2$ are orthogonal, i.e. $$0=\\langle\\vec{v}_2,\\vec{v}_1\\rangle = \\langle\\vec{x}_2,\\vec{v}_1\\rangle - c_{2,1}\\langle\\vec{v}_1,\\vec{v}_1\\rangle.$$ So we set $$c_{2,1} = \\langle\\vec{x}_2,\\vec{v}_1\\rangle/\\|\\vec{v}_1\\|^2.$$\n",
    "3. $\\vec{v}_3 = \\vec{x}_3 - c_{3,1}\\vec{v}_1 -c_{3,2}\\vec{v}_2$. We want to choose the $c$ in such a way that $\\vec{v}_3$ is orthogonal to $\\vec{v}_1$ and $\\vec{v}_2$, i.e. $$0=\\langle\\vec{v}_3,\\vec{v}_1\\rangle = \\langle\\vec{x}_3,\\vec{v}_1\\rangle - c_{3,1}\\langle\\vec{v}_1,\\vec{v}_1\\rangle -c_{3,2}\\langle\\vec{v}_2,\\vec{v}_1\\rangle$$ $$0=\\langle\\vec{v}_3,\\vec{v}_2\\rangle = \\langle\\vec{x}_3,\\vec{v}_2\\rangle - c_{3,1}\\langle\\vec{v}_2,\\vec{v}_1\\rangle -c_{3,2}\\langle\\vec{v}_2,\\vec{v}_2\\rangle$$ Solving for the coefficients we get $$c_{3,1} = \\langle\\vec{x}_3,\\vec{v}_1\\rangle/\\|\\vec{v}_1\\|^2,\\;\\;c_{3,2} = \\langle\\vec{x}_3,\\vec{v}_2\\rangle/\\|\\vec{v}_2\\|^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12a617-89c0-4504-971b-b346f7db5dc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The vectors produced by Gram-Schmidt are orthogonal by construction. They satisfy\n",
    "\n",
    "$$\\vec{v}_1=\\vec{x}_1$$\n",
    "\n",
    "$$\\vec{v}_k = \\vec{x}_k - \\frac{\\langle\\vec{x}_k,\\vec{v}_{1}\\rangle}{\\|\\vec{v}_{1}\\|^2}\\vec{v}_1-\\ldots-\\frac{\\langle\\vec{x}_k,\\vec{v}_{k-1}\\rangle}{\\|\\vec{v}_{k-1}\\|^2}\\vec{v}_{k-1}.$$\n",
    "\n",
    "The above process only produces **orthogonal** vectors; to get **orthonormal** vectors just set $\\vec{u}_k = \\vec{v}_k/\\|\\vec{v}_k\\|$.\n",
    "\n",
    "Notice that $\\vec{v}_k$ is a linear combination of $\\vec{x}_1,\\ldots,\\vec{x}_k$. Furthermore these linear combinations have at least one nonzero coefficient; since the $\\vec{x}_i$ are linearly independent, it's not possible to get $\\vec{v}_k=\\vec{0}$.\n",
    "\n",
    "Gram-Schmidt can be used to test whether a set of vectors is linearly independent; if at any point you get $\\vec{v}_k=\\vec{0}$ then it means that $\\vec{x}_1,\\ldots,\\vec{x}_k$ must be linearly dependent. Of course this only works in exact arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f8594-c935-48df-80e0-fb406e55dd35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Suppose that we put our vectors $\\vec{v}_1,\\ldots,\\vec{v}_{k-1}$ as columns of the matrix $\\mathbf{A}$. The matrix that projects orthogonally onto the range of $\\mathbf{A}$ is\n",
    "\n",
    "$$\\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T.$$\n",
    "\n",
    "$\\mathbf{A}^T\\mathbf{A}$ is the Gram matrix formed from $\\vec{v}_1,\\ldots,\\vec{v}_{k-1}$ using the dot product. Since the vectors are orthogonal, this is a diagona matrix with diagonal entries\n",
    "\n",
    "$$(\\mathbf{A}^T\\mathbf{A})_{ii} = \\|\\vec{v}_i\\|^2.$$\n",
    "\n",
    "The inverse of this matrix is also diagonal with diagonal entries\n",
    "\n",
    "$$(\\mathbf{A}^T\\mathbf{A})_{ii}^{-1} = \\frac{1}{\\|\\vec{v}_i\\|^2}.$$\n",
    "\n",
    "The columns of $\\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}$ are $\\vec{v}_i/\\|\\vec{v}_i\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a7e0f-b23a-4f0a-91d4-cc086ff8518f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Recall from Lecture 1: Any product $\\mathbf{BC}$ can also be written as a sum of outer products\n",
    "\n",
    "$$\\mathbf{BC} = \\vec{b}_1\\otimes\\vec{c}_1 + \\vec{b}_2\\otimes\\vec{c}_2 + \\ldots + \\vec{b}_p\\otimes\\vec{c}_p = \\sum_{k=1}^p\\vec{b}_k\\otimes\\vec{c}_k$$\n",
    "\n",
    "where $\\vec{b}_j$ are the columns of $\\mathbf{B}$ and $\\vec{c}_i$ are the rows of $\\mathbf{C}$. Now apply this using $\\mathbf{B} = \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}$ and $\\mathbf{C} = \\mathbf{A}^T$:\n",
    "\n",
    "$$\\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T = \\frac{1}{\\|\\vec{v}_1\\|^2}\\vec{v}_1\\vec{v}_1^T+\\ldots+\\frac{1}{\\|\\vec{v}_{k-1}\\|^2}\\vec{v}_{k-1}\\vec{v}_{k-1}^T$$\n",
    "\n",
    "The matrix that projects onto the cokernel of $\\mathbf{A}$ is $\\mathbf{I} - \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T$. This is exactly what Gram-Schmidt is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbaf60b-d0e9-4809-8373-f3f0a982f9e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "$$\\vec{v}_k = \\vec{x}_k - \\frac{\\langle\\vec{x}_k,\\vec{v}_{1}\\rangle}{\\|\\vec{v}_{1}\\|^2}\\vec{v}_1-\\ldots-\\frac{\\langle\\vec{x}_k,\\vec{v}_{k-1}\\rangle}{\\|\\vec{v}_{k-1}\\|^2}\\vec{v}_{k-1} = (\\mathbf{I} - \\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T)\\vec{x}_k.$$\n",
    "\n",
    "$\\vec{v}_k$ is the orthogonal projection of $\\vec{x}_k$ onto the orthogonal complement of the span of $\\vec{v}_1,\\ldots,\\vec{v}_{k-1}$. I.e. we get $\\vec{v}_k$ by an orthogonal projection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
