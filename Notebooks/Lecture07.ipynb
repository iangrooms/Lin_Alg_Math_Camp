{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8868c5-d2bf-4d0c-b5ae-14fd90aa8eb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Non-diagonalizable matrices and generalized eigenvectors\n",
    "Some square matrices do not have an eigenvector basis, so they're not diagonalizable.\n",
    "\n",
    "To obtain a basis and associated factorization we need to define 'generalized eigenvectors.' If $\\lambda$ is an eigenvalue, then **generalized eigenvector** is a nonzero vector $\\vec{v}$ such that\n",
    "\n",
    "$$(\\mathbf{A} - \\lambda\\mathbf{I})^k\\vec{v} = \\vec{0}$$\n",
    "\n",
    "for some $k>0$. Eigenvectors are included ($k=1$). The index of a generalized eigenvector is the smallest $k$ such that $(\\mathbf{A} - \\lambda\\mathbf{I})^k\\vec{v} = \\vec{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a04d2-48e8-4d2b-94b0-0f85aa3fb630",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Theorem: Let $\\mathbf{A}$ be $n\\times n$ and $\\lambda$ be an eigenvalue of $\\mathbf{A}$. The null space (kernel) of $(\\mathbf{A} - \\lambda\\mathbf{I})^n$ contains all the generalized eigenvectors associated with $\\lambda$.\n",
    "\n",
    "This is basically telling us that the index of a generalized eigenvector is always $\\le n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c343ac3-aa2c-47a9-a938-0fd35eeafc3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Proof: Suppose that $\\vec{v}$ is a generalized eigenvector, i.e. $(\\mathbf{A}-\\lambda\\mathbf{I})^k\\vec{v} = \\vec{0}$ for some $k>0$. \n",
    "\n",
    "Consider the smallest $k$ such that $(\\mathbf{A}-\\lambda\\mathbf{I})^k\\vec{v} = \\vec{0}$. If $k\\le n$ there is nothing to prove, so suppose that $k>n$, and consider the vectors\n",
    "\n",
    "$$\\vec{v},(\\mathbf{A}-\\lambda\\mathbf{I})\\vec{v},\\ldots,(\\mathbf{A}-\\lambda\\mathbf{I})^{k-1}\\vec{v}.$$\n",
    "\n",
    "These vectors are nonzero by assumption, and we will show that they are linearly independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c4bf1-a0a1-455d-9eac-d11f4d7069ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Consider a linear combination yielding $\\vec{0}$:\n",
    "\n",
    "$$a_0\\vec{v} + a_1(\\mathbf{A}-\\lambda\\mathbf{I})\\vec{v}+\\ldots + a_{k-1}(\\mathbf{A}-\\lambda\\mathbf{I})^{k-1}\\vec{v}=\\vec{0}.$$\n",
    "\n",
    "Multiply by $(\\mathbf{A}-\\lambda\\mathbf{I})^{k-1}$. This annihilates all terms but the first, so $a_0=0$. Then multiply by $(\\mathbf{A}-\\lambda\\mathbf{I})^{k-2}$ and conclude that $a_1=0$. Continuing, we find that all $a_i$ must be zero.\n",
    "\n",
    "We have $k$ linearly independent vectors in an $n$-dimensional space, so we must have $k\\le n$, which proves the theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d71893-e2a8-44d0-8fd8-f6411774fda5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Theorem: Generalized eigenvectors associated with distinct eigenvalues are linearly independent. (Proof omitted; similar to the proof for eigenvectors.)\n",
    "\n",
    "At this point we don't actually know that generalized eigenvectors with index $k>1$ exist. The following theorem says they do:\n",
    "\n",
    "Theorem: The generalized eigenvectors of $\\mathbf{A}$ span the whole space. (Proof omitted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1154e1b-942b-4db6-a38a-b21a52c98b2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Jordan Form\n",
    "\n",
    "Suppose that $\\vec{u}$ is a generalized eigenvector with index $k>1$. Define $\\vec{w} = (\\mathbf{A}-\\lambda\\mathbf{I})^{k-1}\\vec{u}$. Then\n",
    "\n",
    "$$(\\mathbf{A} - \\lambda\\mathbf{I})^k\\vec{u} = (\\mathbf{A} - \\lambda\\mathbf{I})\\vec{w} = \\vec{0}$$\n",
    "\n",
    "so $\\vec{w}$ must be an eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a2c6c-e733-4c76-9728-1605176de3c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Starting with an eigenvector $\\vec{w}_1 = \\vec{v}$, solve\n",
    "\n",
    "$$(\\mathbf{A} - \\lambda\\mathbf{I})\\vec{w}_i = \\vec{w}_{i-1}.$$\n",
    "\n",
    "At some point ($i=k+1$) you won't be able to find a solution any more. The generalized eigenvectors $\\vec{w}_1,\\ldots,\\vec{w}_k$ are a **Jordan chain**.\n",
    "\n",
    "The vectors in a Jordan chain are linearly independent. (Proof omitted.)\n",
    "\n",
    "Consider all the vectors in a Jordan chain and write the vector equations\n",
    "\n",
    "$$\\mathbf{A}\\vec{w}_1 = \\lambda\\vec{w}_1,\\;\\mathbf{A}\\vec{w}_2 = \\lambda\\vec{w}_2+\\vec{w}_1,\\cdots,\\mathbf{A}\\vec{w}_k = \\lambda\\vec{w}_k + \\vec{w}_{k-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c9ca2-e906-49b2-9fc0-e8550d618b16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Write this in matrix form as\n",
    "\n",
    "$$\\mathbf{AW} = \\mathbf{WJ}(\\lambda,k)$$\n",
    "\n",
    "where $\\mathbf{W} = [\\vec{w}_1\\cdots\\vec{w}_k]$ and\n",
    "\n",
    "$$\\mathbf{J}(\\lambda,k) = \\left[\\begin{array}{ccccc}\\lambda&1&0&\\cdots&0\\\\0&\\ddots&\\ddots&\\ddots&\\vdots\\\\\\vdots&\\ddots&\\ddots&\\ddots&0\\\\\\vdots&&\\ddots&\\ddots&1\\\\0&\\cdots&\\cdots&0&\\lambda\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efab0f-f24b-4962-9928-5fc1fa07ca95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The matrix $\\mathbf{W}$ has linearly independent columns, but it's not square, so we can't get to $\\mathbf{A} = \\mathbf{WJ}(\\lambda,k)\\mathbf{W}^{-1}$.\n",
    "\n",
    "To get that kind of a factorization, we write the foregoing matrix equation for **all** the Jordan chains\n",
    "\n",
    "$$\\mathbf{AW}_1=\\mathbf{W}_1\\mathbf{J}(\\lambda_1,k_1),\\cdots,\\mathbf{AW}_m=\\mathbf{W}_m\\mathbf{J}(\\lambda_m,k_m)$$\n",
    "\n",
    "Stacking these equations left to right into a single matrix equation yields\n",
    "\n",
    "$$\\mathbf{AS} = \\mathbf{SJ}$$\n",
    "\n",
    "where $\\mathbf{S} = [\\mathbf{W}_1\\cdots\\mathbf{W}_m]$ and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b905883-b2d1-436e-9a5c-b168d9f01e46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "$$\\mathbf{J} = \\left[\\begin{array}{cccc}\\mathbf{J}(\\lambda_1,k_1)&0&\\cdots&0\\\\0&\\mathbf{J}(\\lambda_2,k_1)&0&\\\\\\vdots&&\\ddots&\\vdots\\\\0&\\cdots&\\cdots&\\mathbf{J}(\\lambda_m,k_m)\\end{array}\\right].$$\n",
    "\n",
    "The columns of $\\mathbf{S}$ are linearly independent and the matrix is square so we can write\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{SJS}^{-1}.$$\n",
    "\n",
    "This is called the **Jordan form** (or sometimes the Jordan canonical form). Sometimes just the **J** matrix is called the Jordan form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe7892-e6ad-465c-8695-e580302e441e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Functions of matrices\n",
    "\n",
    "In the broadest sense, a 'function of a matrix' is a function that takes a matrix as an argument. That is not the _usual_ sense of the term though. \n",
    "\n",
    "The _usual_ meaning of 'function of a matrix' is that we have some function $f:\\mathbb{C}\\to\\mathbb{C}$ and we want to define another function $g:\\mathbb{C}^{n\\times n}\\to\\mathbb{C}^{n\\times n}$ that has special properties that connect it closely to $f$.\n",
    "\n",
    "The two functions are so closely connected that we will abuse notation and say $f=g$, e.g. if$f(x) = e^x$ then we will say $g(\\mathbf{A}) = f(\\mathbf{A}) = e^\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84690b-5337-4cd9-ba54-a0114b5a6e26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Consider an expression like\n",
    "\n",
    "$$(a_0\\mathbf{I} + a_1\\mathbf{A} + \\cdots + a_m\\mathbf{A}^m)\\vec{v}.$$\n",
    "\n",
    "It seems very natural to write\n",
    "\n",
    "$$p(\\mathbf{A}) = a_0\\mathbf{I} + a_1\\mathbf{A} + \\cdots + a_m\\mathbf{A}^m$$\n",
    "\n",
    "which connects a scalar function $p(x) = a_0 + a_1x + \\cdots a_mx^m$ with a matrix function. (Note that the scalar function is *not* applied element-by-element.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c6032-c613-42f5-b575-83df94760d84",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Continuing the foregoing example, suppose that **A** is diagonalizable, i.e.\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{S\\Lambda S}^{-1}.$$\n",
    "\n",
    "Plugging this in and simplifying yields\n",
    "\n",
    "$$p(\\mathbf{A}) = \\mathbf{S}\\left(a_0\\mathbf{I} + a_1\\mathbf{\\Lambda} + \\cdots + a_m\\mathbf{\\Lambda}^m\\right)\\mathbf{S}^{-1}.$$\n",
    "\n",
    "Applying $p$ to the diagonal matrix $\\mathbf{\\Lambda}$ just applies $p$ to the diagonal elements of $\\mathbf{\\Lambda}$, i.e. the eigenvalues of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c140eed-88e0-451f-83bd-9370552b845a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Going further, suppose that we have a convergent power series representation of a function $f$\n",
    "\n",
    "$$f(x) = \\sum_{n=0}^\\infty a_n (x - c)^n$$\n",
    "\n",
    "If $\\mathbf{A}$ is diagonalizable we can write\n",
    "\n",
    "$$f(\\mathbf{A}) = \\mathbf{S}\\left(\\sum_{n=0}^\\infty a_n(\\mathbf{\\Lambda} - c\\mathbf{I})^n\\right)\\mathbf{S}^{-1} = \\mathbf{S}f(\\mathbf{\\Lambda})\\mathbf{S}^{-1}$$\n",
    "\n",
    "where the final $f$ is applied to the diagonal elements of $\\mathbf{\\Lambda}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f243f7c-4b8d-4cb9-809f-6ac9d4b6a306",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "This isn't a good _definition_ of a matrix function though, because\n",
    "- It only works if the power series is convergent at all the eigenvalues of **A**, and\n",
    "- It isn't defined for non-diagonalizable matrices.\n",
    "\n",
    "What happens if we apply the power series approach to a non-diagonalizable matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59424a5-83bf-4916-987c-710161e7d0c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Consider a single Jordan block\n",
    "\n",
    "$$\\mathbf{J}(\\lambda,k) = \\lambda\\mathbf{I} + \\mathbf{N}_k$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathbf{N}_k = \\left[\\begin{array}{cccc}0&1&&\\\\&\\ddots&\\ddots&\\\\&&\\ddots&1\\\\&&&0\\end{array}\\right]$$\n",
    "\n",
    "Notice that $\\mathbf{N}_k^k = \\mathbf{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e71a1-c40c-4efd-929e-42d3a3876d93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "So for $n\\ge k-1$\n",
    "\n",
    "$$(\\lambda\\mathbf{I} + \\mathbf{N}_k)^n = \\lambda^n\\mathbf{I} + \\left(\\begin{array}{c}n\\\\1\\end{array}\\right)\\lambda^{n-1}\\mathbf{N}_k + \\cdots + \\left(\\begin{array}{c}n\\\\k-1\\end{array}\\right)\\lambda^{n-k+1}\\mathbf{N}_k^{k-1}$$\n",
    "\n",
    "If we sum the Taylor series we find\n",
    "\n",
    "$$f(\\mathbf{J}(\\lambda,k)) = \\left[\\begin{array}{ccccc}f(\\lambda)&f'(\\lambda)&\\frac{f''(\\lambda)}{2!}&\\cdots&\\frac{f^{(k-1)}(\\lambda)}{(k-1)!}\\\\0&\\ddots&\\ddots&\\ddots&\\vdots\\\\\\vdots&\\ddots&\\ddots&\\ddots&\\frac{f''(\\lambda)}{2!}\\\\\\vdots&&\\ddots&\\ddots&f'(\\lambda)\\\\0&\\cdots&\\cdots&0&f(\\lambda)\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008318a1-b4ae-4240-87a4-c778e9d10390",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Definition of a (primary) function of a matrix:\n",
    "\n",
    "Suppose that $\\mathbf{A}$ has eigenvalues $\\lambda_i$, each with a Jordan block of size $k_i$. Suppose that $f^{(j)}(\\lambda_i)$ exists for all $i$ and for all $j=1,\\ldots,k_i-1$. Then we define $f(\\mathbf{A})$ by applying the function $f$ to the Jordan blocks of $\\mathbf{A}$ as on the previous slide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd883b3e-479c-4e5f-b597-a06a9fd10481",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If $f(\\mathbf{A})$ and $g(\\mathbf{A})$ are well-defined, then \n",
    "$$f(\\mathbf{A})g(\\mathbf{A})=g(\\mathbf{A})f(\\mathbf{A})$$\n",
    "\n",
    "$$f(\\mathbf{A}^T) = f(\\mathbf{A})^T$$\n",
    "\n",
    "If $\\mathbf{B}$ commutes with $\\mathbf{A}$ then $\\mathbf{B}$ commutes with $f(\\mathbf{A})$\n",
    "\n",
    "If $\\mathbf{A}$ is upper triangular then $f(\\mathbf{A})$ is also upper triangular and $f(\\mathbf{A})_{ii} = f(a_{ii})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c18cf-3c46-47a6-b7ad-7c636a3eec3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The **spectral radius** $\\rho(\\mathbf{A})$ of a matrix **A** is the largest absolute value of the eigenvalues of **A**.\n",
    "\n",
    "The spectral radius is not a norm. But it has an interesting relation to norms.\n",
    "\n",
    "Theorem: For any matrix operator norm $\\|\\cdot\\|$ we have\n",
    "\n",
    "$$\\rho(\\mathbf{A})\\le\\|\\mathbf{A}\\|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5a33a-ca21-4c3e-a8ea-d8b25fb04b80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Proof: Recall that $\\|\\mathbf{A}\\| = \\max_{\\|\\vec{u}\\|=1}\\|\\mathbf{A}\\vec{u}\\|$.\n",
    "\n",
    "Let $\\vec{u}$ be a unit vector such that $\\mathbf{A}\\vec{u} = \\lambda_{max}\\vec{u}$ where $\\rho(\\mathbf{A}) = |\\lambda_{max}|$.\n",
    "\n",
    "Clearly\n",
    "\n",
    "$$\\rho(\\mathbf{A}) = |\\lambda_{max}| = \\|\\lambda_{max}\\vec{u}\\| = \\|\\mathbf{A}\\vec{u}\\|\\le\\|\\mathbf{A}\\|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f057737-02da-4244-a4de-7eaa5a09c4b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "There is also an interesting result that for any $\\epsilon>0$ and any **A** there is a matrix norm $\\|\\cdot\\|$ such that \n",
    "\n",
    "$$\\rho(\\mathbf{A})\\le \\|\\mathbf{A}\\|\\le\\rho(\\mathbf{A}) + \\epsilon.$$\n",
    "\n",
    "The spectral radius is thus the infimum of $\\|\\mathbf{A}\\|$ over all matrix norms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33928b-303f-471e-aeea-61dd0fbd5f24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Proof (Horn & Johnson 5.6.10): Consider the Jordan decomposition\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{SJS}^{-1}.$$\n",
    "\n",
    "Define $\\mathbf{D}_t = $diag$(t,t^2,\\ldots,t^n)$ for $t>0$, and then define the matrix norm\n",
    "\n",
    "$$\\|\\mathbf{A}\\|:= \\|\\mathbf{D}_t\\mathbf{S}^{-1}\\mathbf{ASD}_t^{-1}\\|_1$$\n",
    "\n",
    "(This is a norm and is submultiplicative; therefore is a matrix norm.) Note that the matrix whose 1-norm we are taking is block-diagonal with blocks in the form\n",
    "\n",
    "$$\\left[\\begin{array}{ccccc}\\lambda&t^{-1}&0&\\cdots&0\\\\0&\\ddots&\\ddots&\\ddots&\\vdots\\\\\\vdots&\\ddots&\\ddots&\\ddots&0\\\\\\vdots&&\\ddots&\\ddots&t^{-1}\\\\0&\\cdots&\\cdots&0&\\lambda\\end{array}\\right].$$\n",
    "The max absolute column sum is $\\rho(\\mathbf{A})+t^{-1}$, so by taking $t^{-1}<\\epsilon$ we can make $\\|\\mathbf{A}\\|<\\rho(\\mathbf{A})+\\epsilon$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
