{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Vector Norms\n",
    "The standard definition of the 'length' of a vector in $\\mathbb{R}^n$ is\n",
    "\n",
    "$$\\|\\vec{x}\\|=\\sqrt{\\sum_i x_i^2}.$$\n",
    "\n",
    "It is valuable to generalize the notion of length to arbitrary vector spaces. A **norm** is a function that takes a vector as input and returns a scalar. It has the following properties\n",
    "1. $\\|\\vec{x}\\|\\ge0$ for all $\\vec{x}$ and $=0$ only when $\\vec{x} = \\vec{0}$\n",
    "2. $\\|\\alpha\\vec{x}\\| = |\\alpha|\\|\\vec{x}\\|$ for every scalar $\\alpha$ and vector $\\vec{x}$\n",
    "3. $\\|\\vec{x}+\\vec{y}\\|\\le\\|\\vec{x}\\|+\\|\\vec{y}\\|$ for every $\\vec{x}$ and $\\vec{y}$ (triangle inequality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here are a few standard norms for vectors in $\\mathbb{R}^n$:\n",
    "1. The 1-norm $$\\|\\vec{x}\\|_1 = |x_1| + |x_2| + \\ldots + |x_n|$$\n",
    "2. The 2-norm $$\\|\\vec{x}\\|_2 = \\sqrt{\\vec{x}\\cdot\\vec{x}} = \\left(x_1^2+\\ldots+x_n^2\\right)^{1/2}$$\n",
    "3. The $p$-norm $$\\|\\vec{x}\\|_p = \\left(|x_1|^p+\\ldots+|x_n|^p\\right)^{1/p}$$ for $1\\le p<\\infty$\n",
    "4. The $\\infty$-norm $$\\|\\vec{x}\\|_\\infty = \\max\\{|x_1|,|x_2|,\\ldots,|x_n|\\}.$$\n",
    "\n",
    "A **unit vector** is a vector whose norm is $1$. A vector might be a unit vector in one kind of norm but not another norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Unit Vectors](./OS3p5.png)\n",
    "\n",
    "A **unit sphere** is the set of all unit vectors for a given norm, and a **unit ball** is the set of all vectors with norm $\\le 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Equivalence of Norms\n",
    "If $V$ is a finite-dimensional vector space and $\\|\\cdot\\|_A$ and $\\|\\cdot\\|_B$ are norms on $V$, then there are constants $c,C>0$ such that\n",
    "\n",
    "$$c\\|\\vec{x}\\|_A\\le \\|\\vec{x}\\|_B\\le C\\|\\vec{x}\\|_A$$\n",
    "\n",
    "for every $\\vec{x}\\in V$. Proof is based on compactness of the unit sphere in finite dimensions and on continuity of the norm. Cf. the analysis math camp.\n",
    "\n",
    "You can find these coefficients using Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Matrix Norms\n",
    "The set of linear functions between finite-dimensional vector spaces is itself a finite-dimensional vector space, and you can define norms on that vector space if you want to.\n",
    "\n",
    "We distinguish between norms on matrices that don't know that they're matrices (i.e. they just treat a matrix as a vector in a high-dimensional space), and norms that are somehow tied to the fact that it's a matrix.\n",
    "\n",
    "The confusing terminology is that the former are 'vector norms on matrices' while the latter are 'matrix norms.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For example, if you have $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ and you just think of it like a list of $mn$ numbers, you can apply the vector 2-norm:\n",
    "\n",
    "$$\\|\\mathbf{A}\\|_F^2 = \\sum_{i=1}^m\\sum_{j=1^n}a_{ij}^2.$$\n",
    "\n",
    "This is called the Frobenius norm. It is a 'vector norm on matrices' and is *not* the same as the matrix 2-norm (TBD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Matrix norms** are functions $\\mathbb{C}^{n\\times n}\\to\\mathbb{R}$ that satisfy the usual 3 properties of a norm **and** the submultiplicative property\n",
    "\n",
    "$$\\|\\mathbf{AB}\\|\\le \\|\\mathbf{A}\\|\\|\\mathbf{B}\\|.$$\n",
    "\n",
    "A special kind of norm on matrices (and on linear operators between vector spaces) is an **operator norm**:\n",
    "\n",
    "$$\\|\\mathbf{A}\\| = \\max_{\\|\\vec{u}\\|=1}\\|\\mathbf{A}\\vec{u}\\|.$$\n",
    "\n",
    "This is a matrix norm and also satisfies:\n",
    "\n",
    "$$\\|\\mathbf{A}\\vec{x}\\|\\le\\|\\mathbf{A}\\|\\|\\vec{x}\\|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "1. The 1-norm of a square matrix is $\\|\\mathbf{A}\\|_1 = \\max_j\\sum_i|a_{ij}|$ the max absolute column sum. Sometimes people confusingly use this notation to mean the sum of the absolute values of the entries of the matrix.\n",
    "2. The $\\infty$-norm of a square matrix is $\\|\\mathbf{A}\\|_\\infty = \\max_i\\sum_j|a_{ij}|$ the max absolute row sum. Sometimes people confusingly use this notation to mean the max of the absolute values of the entries of the matrix.\n",
    "\n",
    "The 2-norm of a matrix is not the same as the Frobenius norm\n",
    "\n",
    "$$\\|\\mathbf{A}\\|_2 = \\max_{\\|\\vec{u}\\|_2=1}\\|\\mathbf{A}\\vec{u}\\|_2.$$\n",
    "\n",
    "It is sometimes confusingly called the spectral norm of the matrix; please don't call it that. We will come back to the matrix 2-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Inner products\n",
    "The concept of an inner product generalizes/abstracts the dot product. A general inner product takes two vectors as input and returns a scalar. It has the following properties:\n",
    "1. $\\langle\\vec{x},\\vec{x}\\rangle \\ge0$ for all $\\vec{x}$ and $\\langle\\vec{x},\\vec{x}\\rangle=0$ only for $\\vec{x}=\\vec{0}$\n",
    "2. $\\langle\\vec{x},\\vec{y}\\rangle = \\langle\\vec{y},\\vec{x}\\rangle$ \n",
    "3. $$\\langle\\alpha\\vec{x}+\\beta\\vec{y},\\vec{z}\\rangle = \\alpha\\langle\\vec{x},\\vec{z}\\rangle + \\beta\\langle\\vec{y},\\vec{z}\\rangle$$\n",
    "\n",
    "The definition is slightly different when the vectors are complex: property 2 becomes $\\langle\\vec{x},\\vec{y}\\rangle = \\overline{\\langle\\vec{y},\\vec{x}\\rangle}$ where the overbar denotes complex conjugation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let $\\mathbf{K}$ be a real matrix with the following properties:\n",
    "1. $\\mathbf{K} = \\mathbf{K}^T$ (symmetric)\n",
    "2. $\\vec{x}^T\\mathbf{K}\\vec{x}\\ge0$ and $=0$ only for $\\vec{x} = \\vec{0}$. (positive definite)\n",
    "\n",
    "Then the following defines an inner product on $\\mathbb{R}^n$\n",
    "\n",
    "$$\\langle\\vec{x},\\vec{y}\\rangle = \\vec{x}^T\\mathbf{K}\\vec{y}.$$\n",
    "\n",
    "It is straightforward to check that this satisfies the three properties of an inner product.\n",
    "Symmetry is instructive: $\\vec{x}^T\\mathbf{K}\\vec{y}$ is just a number, so it has to be equal to its transpose:\n",
    "$$\\vec{x}^T\\mathbf{K}\\vec{y}=(\\vec{x}^T\\mathbf{K}\\vec{y})^T.$$\n",
    "Now use the properties of transposes and use that the matrix is symmetric:\n",
    "$$\\vec{x}^T\\mathbf{K}\\vec{y}=(\\vec{x}^T\\mathbf{K}\\vec{y})^T = \\vec{y}^T\\mathbf{K}^T\\vec{x} = \\vec{y}^T\\mathbf{K}\\vec{x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Every inner product on $\\mathbb{R}^n$ can be written in the form \n",
    "\n",
    "$$\\langle\\vec{x},\\vec{y}\\rangle = \\vec{x}^T\\mathbf{K}\\vec{y}$$\n",
    "\n",
    "for some matrix $\\mathbf{K}$ that is symmetric and positive definite.\n",
    "The proof is constructive, i.e. it shows you how to get the entries of the matrix $\\mathbf{K}$.\n",
    "First expand $\\vec{x}$ and $\\vec{y}$ in the standard basis:\n",
    "\n",
    "$$\\vec{x} = x_1\\vec{e}_1 + \\ldots + x_n\\vec{e}_n = \\sum_{i=1}^nx_i\\vec{e}_i, \\;\\vec{y} = y_1\\vec{e}_1 + \\ldots + y_n\\vec{e}_n  = \\sum_{j=1}^ny_j\\vec{e}_j.$$\n",
    "\n",
    "Now plug in and use properties of the inner product:\n",
    "$$\\langle\\vec{x},\\vec{y}\\rangle = \\sum_{ij}x_iy_j\\langle\\vec{e}_i,\\vec{e}_j\\rangle.$$\n",
    "The expression $\\vec{x}^T\\mathbf{K}\\vec{y}$ can also be written\n",
    "$$\\sum_{ij}x_iy_jK_{ij}.$$\n",
    "Matching terms shows that\n",
    "$$K_{ij} = \\langle\\vec{e}_i,\\vec{e}_j\\rangle.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Every inner product on $\\mathbb{C}^n$ can be written\n",
    "\n",
    "$$\\langle\\vec{x},\\vec{y}\\rangle = \\vec{x}^T\\mathbf{K}\\vec{\\bar{y}}$$\n",
    "\n",
    "where $\\mathbf{K}$ satisfies\n",
    "1. $\\mathbf{K} = \\bar{\\mathbf{K}}^T := \\mathbf{K}^*$ or $\\mathbf{K}^H$ (conjugate symmetric, aka Hermitian)\n",
    "2. $\\vec{x}^T\\mathbf{K}\\vec{\\bar{x}}\\ge0$ and $=0$ only for $\\vec{x} = \\vec{0}$. (positive definite)\n",
    "\n",
    "The complex dot product uses $\\mathbf{K} = \\mathbf{I}$:\n",
    "\n",
    "$$\\vec{x}\\cdot\\vec{y} = \\vec{x}^T\\vec{\\bar{y}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Gram matrices\n",
    "Suppose you have a set of vectors $\\vec{v}_1,\\ldots,\\vec{v}_n$, and an inner product $\\langle\\cdot,\\cdot\\rangle$.\n",
    "If you construct a matrix $\\mathbf{K}$ with the following entries\n",
    "\n",
    "$$K_{ij} = \\langle\\vec{v}_i,\\vec{v}_j\\rangle$$\n",
    "\n",
    "then this matrix is called a **Gram** matrix. It is always symmetric because the inner product is symmetric.\n",
    "\n",
    "Furthermore, if the vectors are linearly independent then $\\mathbf{K}$ is positive definite, and if they are not independent then it is positive semi-definite.\n",
    "\n",
    "The following proofs for $\\mathbb{R}^n$ can be extended to arbitrary finite-dimensional vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose that your inner product is just the dot product. Then $K_{ij} = \\vec{v}_i^T\\vec{v}_j$. Construct a matrix $\\mathbf{A}$ whose columns are the vectors $\\vec{v}_1,\\ldots,\\vec{v}_n$; then \n",
    "\n",
    "$$\\mathbf{K} = \\mathbf{A}^T\\mathbf{A}.$$\n",
    "\n",
    "(Recall that the $ij$ entry of a matrix product is the $i^\\text{th}$ row of the left matrix dotted with the $j^\\text{th}$ column of the right matrix.)\n",
    "\n",
    "Now examine the function\n",
    "$$\\vec{x}^T\\mathbf{K}\\vec{x} = \\vec{x}^T\\mathbf{A}^T\\mathbf{A}\\vec{x} = \\left(\\mathbf{A}\\vec{x}\\right)^T\\mathbf{A}\\vec{x} = \\|\\mathbf{A}\\vec{x}\\|_2^2\\ge0.$$\n",
    "How do we know whether it's positive definite or just semi-definite? Clearly the quadratic form is zero only when $\\mathbf{A}\\vec{x}=0$. If there's a $\\vec{x}\\neq0$ such that $\\mathbf{A}\\vec{x}=\\vec{0}$ then the columns of $\\mathbf{A}$ are linearly dependent; otherwise the columns are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now suppose you're using an arbitrary inner product on $\\mathbb{R}^n$ of the form $\\langle\\vec{x},\\vec{y}\\rangle = \\vec{x}^T\\mathbf{C}\\vec{y}$ ($\\mathbf{C}$ is SPD). Then the entries of the Gram matrix $\\mathbf{K}$ are $K_{ij} = \\vec{v}_i^T\\mathbf{C}\\vec{v}_j$. Construct a matrix $\\mathbf{A}$ whose columns are the vectors $\\vec{v}_1,\\ldots,\\vec{v}_n$; then \n",
    "\n",
    "$$\\mathbf{K} = \\mathbf{A}^T\\mathbf{C}\\mathbf{A}.$$\n",
    "\n",
    "Now examine the quadratic form\n",
    "$$\\vec{x}^T\\mathbf{K}\\vec{x} = \\vec{x}^T\\mathbf{A}^T\\mathbf{CA}\\vec{x} = \\left(\\mathbf{A}\\vec{x}\\right)^T\\mathbf{C}\\left(\\mathbf{A}\\vec{x}\\right) = \\langle\\mathbf{A}\\vec{x},\\mathbf{A}\\vec{x}\\rangle\\ge0.$$\n",
    "How do we know whether it's positive definite or just semi-definite? By the properties of inner products we know that the inner product of $\\mathbf{A}\\vec{x}$ with itself is zero only when $\\mathbf{A}\\vec{x}=0$. If there's a $\\vec{x}\\neq0$ such that $\\mathbf{A}\\vec{x}=\\vec{0}$ then the columns of $\\mathbf{A}$ are linearly dependent; otherwise the columns are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Cholesky Factorization\n",
    "Real symmetric matrices **K** that are positive definite ($\\vec{x}^T\\mathbf{K}\\vec{x}>0$ for $\\vec{x}\\neq\\vec{0}$) do not require row swaps during Gaussian elimination (proof in APPM 5600 or 5620).\n",
    "\n",
    "They therefore have an LU factorization without permuting, which makes the factorization unique\n",
    "\n",
    "$$\\mathbf{K} = \\mathbf{LU}.$$\n",
    "\n",
    "(You can still permute for various reasons, you just don't **need** to to avoid zeros on the diagonal.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let $\\mathbf{D}$ be the diagonal of the REF **U**. Symmetry then implies that\n",
    "\n",
    "$$\\mathbf{K} = \\mathbf{LDL}^T.$$\n",
    "\n",
    "Positive-definiteness implies that the diagonal elements of **D** must all be positive (proof left as exercise). We can therefore take a positive square root of each diagonal element and write\n",
    "\n",
    "$$\\mathbf{K} = \\mathbf{LD}^{1/2}\\left(\\mathbf{LD}^{1/2}\\right)^T.$$\n",
    "\n",
    "This is the Cholesky factorization $\\mathbf{K} = \\mathbf{GG}^T$ where $\\mathbf{G} = \\mathbf{LD}^{1/2}$. Usually the factor is denoted **L** rather than **G**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Adjoints\n",
    "\n",
    "The adjoint of a square matrix $\\mathbf{A}$ with respect to an inner product is defined to be the matrix $\\mathbf{A}^*$ that satisfies\n",
    "\n",
    "$$\\langle\\mathbf{A}^*\\vec{x},\\vec{y}\\rangle = \\langle\\vec{x},\\vec{y}\\rangle\\quad \\forall \\vec{x},\\vec{y}.$$\n",
    "\n",
    "If the vectors are real and the inner product is the dot product then we are looking for a matrix $\\mathbf{A}^*$ that satisfies\n",
    "\n",
    "$$(\\mathbf{A}^*\\vec{x})^T\\vec{y} = \\vec{x}^T\\mathbf{A}\\vec{y}.$$\n",
    "\n",
    "For this inner product the adjoint is just the transpose: $\\mathbf{A}^*=\\mathbf{A}^T$. For the complex dot product the adjoint is the complex-conjugate transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A general inner product on $\\mathbb{R}^n$ has the form\n",
    "\n",
    "$$\\langle\\vec{x},\\vec{y}\\rangle = \\vec{x}^T\\mathbf{K}\\vec{y}$$\n",
    "\n",
    "for some SPD matrix $\\mathbf{K}$. What is the adjoint of $\\mathbf{A}$ with respect to this inner product? It is a matrix that satisfies\n",
    "\n",
    "$$(\\mathbf{A}^*\\vec{x})^T\\mathbf{K}\\vec{y} = \\vec{x}\\mathbf{KA}\\vec{y}\\quad\\forall\\vec{x},\\vec{y}.$$\n",
    "\n",
    "The solution is\n",
    "\n",
    "$$\\mathbf{A}^* = \\mathbf{K}^{-1}\\mathbf{A}^T\\mathbf{K}.$$\n",
    "\n",
    "A self-adjoint matrix equals its adjoint. A symmetric matrix is self-adjoint with respect to the real dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Riesz Representation Theorem\n",
    "\n",
    "The Riesz representation theorem is something you see in the graduate analysis courses dealing with infinite-dimensional inner-product spaces. The finite-dimensional version is still instructive.\n",
    "\n",
    "The dual space $V^*$ of a vector space $V$ is the space of linear functions from $V$ to $\\mathbb{R}$ (or to $\\mathbb{C}$ for complex vector spaces).\n",
    "\n",
    "We know that if $V$ has dimension $n$ then the dual space also has dimension $n$, and each element of the dual space can be identified with a vector of length $n$ by picking a basis for $V$.\n",
    "\n",
    "The Riesz representation theorem says that every (continuous$^*$) linear function from $V$ to $\\mathbb{R}$ (or to $\\mathbb{C}$ for complex vector spaces) can be uniquely identified with an element $\\vec{y}$ of the vector space so that\n",
    "\n",
    "$$L[\\vec{u}] = \\langle\\vec{u},\\vec{w}\\rangle\\quad\\forall \\vec{u}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "To find the vector $\\vec{w}$ that corresponds to a linear form $L$, first pick a basis $\\vec{v}_1,\\ldots,\\vec{v}_n$ of $V$. Expand both $\\vec{u}$ and $\\vec{w}$ in this basis:\n",
    "\n",
    "$$\\vec{u} = x_1\\vec{v}_1 + \\ldots+x_n\\vec{v}_n,\\quad \\vec{w} = y_1\\vec{v}_1 + \\ldots+y_n\\vec{v}_n.$$\n",
    "\n",
    "The inner product of these two vectors is (for real vector spaces)\n",
    "\n",
    "$$\\langle\\vec{u},\\vec{w}\\rangle = \\vec{x}^T\\mathbf{K}\\vec{y}.$$\n",
    "\n",
    "We also have\n",
    "\n",
    "$$L[\\vec{u}] = x_1L[\\vec{v}_1] + \\ldots + x_nL[\\vec{v}_n] = \\vec{x}^T\\left(\\begin{array}{c}L[\\vec{v}_1]\\\\\\vdots\\\\L[\\vec{v}_n]\\end{array}\\right) = \\vec{x}^T\\vec{l}.$$\n",
    "\n",
    "For these to be equal we must have $\\vec{y} = \\mathbf{K}^{-1}\\vec{l}$. Note that $\\vec{y}\\in\\mathbb{R}^n$ is not equal to $\\vec{w}\\in V$. Why is $\\mathbf{K}$ invertible?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
